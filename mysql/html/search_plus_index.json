{"./":{"url":"./","title":"Introduction","keywords":"","body":"Mysql "},"chapter1.html":{"url":"chapter1.html","title":"mysql","keywords":"","body":"mysql "},"luo-ji-jia-gou-tu.html":{"url":"luo-ji-jia-gou-tu.html","title":"连接管理","keywords":"","body":"每个客户端连接都会在服务器进程中拥有一个线程，这个连接的查询只会在这个单独的线程中执行，该线程只能轮流在某个CPU核心或者CPU中运行。服务器会负责缓存线程，因此不需要为每一个新建的连接创建或者销毁线程 。当客户端（应用）连接到MySQL服务器时，服务器需要对其进行认证。认证基于用户名、原始主机信息和密码。如果使用了安全套接字（SSL）的方式连接，还可以使用X.509证书认证。一旦客户端连接成功，服务器会继续验证该客户端是否具有执行某个特定查询的权限（例如，是否允许客户端对world数据库的Country表执行SELECT语句）。 "},"jie-xi-qi.html":{"url":"jie-xi-qi.html","title":"解析器","keywords":"","body":"对于SELECT语句，在解析查询之前，服务器会先检查查询缓存（QueryCache），如果能够在其中找到对应的查询，服务器就不必再执行查询解析、优化和执行的整个过程，而是直接返回查询缓存中的结果集。 "},"huan-cun-yi-ji-ming-zhong-lv.html":{"url":"huan-cun-yi-ji-ming-zhong-lv.html","title":"缓存","keywords":"","body":""},"huan-cun-yi-ji-ming-zhong-lv/innodbcun-chu-yin-qing-de-huan-chong-chi.html":{"url":"huan-cun-yi-ji-ming-zhong-lv/innodbcun-chu-yin-qing-de-huan-chong-chi.html","title":"InnoDB存储引擎的缓冲池","keywords":"","body":"InnoDB存储引擎的缓冲池，通常InnoDB存储引擎缓冲池的命中不应该小于99%。 参数说明： Innodb_buffer_pool_reads: 表示从物理磁盘读取页的次数 Innodb_buffer_pool_read_ahead: 预读的次数 Innodb_buffer_pool_read_ahead_evicted: 预读的页，但是没有读取就从缓冲池中被替换的页的数量，一般用来判断预读的效率 Innodb_buffer_pool_read_requests: 从缓冲池中读取页的次数 Innodb_data_read: 总共读入的字节数 Innodb_data_reads: 发起读取请求的次数，每次读取可能需要读取多个页 innodb_buffer_pool_size=1G "},"huan-cun-yi-ji-ming-zhong-lv/cha-xun-huan-cun.html":{"url":"huan-cun-yi-ji-ming-zhong-lv/cha-xun-huan-cun.html","title":"查询缓存","keywords":"","body":"查询缓存的工作原理，基本上可以概括为： 缓存SELECT操作或预处理查询（注释：5.1.17开始支持）的结果集和SQL语句； 新的SELECT语句或预处理查询语句，先去查询缓存，判断是否存在可用的记录集，判断标准：与缓存的SQL语句，是否完全一样，区分大小写； 查询缓存对什么样的查询语句，无法缓存其记录集，大致有以下几类： SQL_NO_CACHE参数； 查询语句中含有获得值的函数，包涵自定义函数，如：CURDATE()、GET_LOCK()、RAND()、CONVERT_TZ等； 对系统数据库的查询：mysql、information_schema 查询语句中使用SESSION级别变量或存储过程中的局部变量； 查询语句中使用了LOCK IN SHARE MODE、FOR UPDATE的语句 查询语句中类似SELECT …INTO 导出数据的语句； 对临时表的查询操作； 存在警告信息的查询语句； 不涉及任何表或视图的查询语句； 某用户只有列级别权限的查询语句； 事务隔离级别为：Serializable情况下，所有查询语句都不能缓存； 配置 #0(OFF)关闭 Query Cache 功能，任何情况下都不会使用 Query Cache; #1(ON)启用查询缓存，只要符合查询缓存的要求，客户端的查询语句和记录集斗可以； #2(DEMAND)启用查询缓存，只有查询语句中使用了参数：sql_cache，且符合查询缓存的要求； query_cache_type=0 #query_cache_size的值最小为40K query_cache_size=64M query_cache_size 允许设置query_cache_size的值最小为40K，对于最大值则可以几乎认为无限制，实际生产环境的应用经验告诉我们，该值并不是越大， 查询缓存的命中率就越高，也不是对服务器负载下降贡献大，反而可能抵消其带来的好处，甚至增加服务器的负载，推荐设置为：64M；建议设置不要超过256MB show global status like 'QCache%'; -- 目前还处于空闲状态的 Query Cache 中内存 Block 数目 'Qcache_free_blocks','6' -- 前还处于空闲状态的 Query Cache 内存总量 'Qcache_free_memory','63114352' --Query Cache 命中次数 'Qcache_hits','51729' -- 向 Query Cache 中插入新的 Query Cache 的次数，也就是没有命中的次数 'Qcache_inserts','15398' -- 当 Query Cache 内存容量不够，需要从中删除老的 Query Cache 以给新的 Cache 对象使用的次数 'Qcache_lowmem_prunes','0' -- 没有被 Cache 的 SQL 数，包括无法被 Cache 的 SQL 以及由于 query_cache_type 设置的不会被 Cache 的 SQL 'Qcache_not_cached','3008' -- 目前在 Query Cache 中的 SQL 数量 'Qcache_queries_in_cache','2006' -- Query Cache 中总的 Block 数量 'Qcache_total_blocks','4058' 内存碎片的产生。当一块分配的内存没有完全使用时，MySQL会把这块内存Trim掉，把没有使用的那部分归还以重 复利用。比如，第一次分配4KB,只用了3KB，剩1KB，第二次连续操作，分配4KB，用了2KB，剩2KB，这两次连续操作共剩下的 1KB+2KB=3KB，不足以做个一个内存单元分配， 这时候，内存碎片便产生了。使用flush query cache，可以消除碎片 --消除碎片 flush query cache 命中率和内存使用率算法 query_cache_min_res_unit的估计值：(query_cache_size - Qcache_free_memory) / Qcache_queries_in_cache 查询缓存命中率 ≈ (Qcache_hits – Qcache_inserts) / Qcache_hits * 100% 查询缓存内存使用率 ≈ (query_cache_size – Qcache_free_memory) / query_cache_size * 100% 一是所采用的SQL文本是相同的。当前后两次用户使用了相同的SQL语句(假设不考虑其他条件)，则服务器会从缓存中读取结果，而不需要再去解析和执行SQL语句。这里需要注意的是，这里的SQL文本必须一次不差的完全相同。如果前后两次查询，使用了不同的查询条件。如第一次查询时没有输入Where条件语句。后来发现数据量过多，利用了Where条件了过滤查询的结果。此时即使最后的查询结果是相同的，系统仍然是从数据文件中获取数据，而不是从数据缓存中。再如，Select后面所使用的字段名称也必须是相同的。如果有一个字段名称不同或者前后两次查询所使用的字段数量不同，则系统都会认为是不同的SQL语句，而重新解析并查询。 二是从数据缓存的角度考虑，大小写是不敏感的。如前后两次查询时，采用的字段名称可能只有大小写的差异。如第一次使用的是大小，第二次使用的是小写，这系统认为仍然是相同的SQL语句。或者说关键字大小写等等这都是不敏感的。 三是要满足二次查询之间，数据记录包括表结构都没有被更改过。如果记录所在的标更改了，如增加了一个字段等等，此时使用这个表的所有缓冲数据系统将自动清空。这里需要注意，这里指的更改是一个广义的更改，包括表中任何数据或者结果的改变。举一个简单的例子，第一次查询时用户需要查询2010年的出货数据。查询后有用户在这个表中插入了一条2011年1月份的出货信息。然后又有用户需要查询2010年的出货信息。使用的SQL语句与第一次查询时完全相同。在这种情况下，数据库系统会使用缓存中的数据吗?答案是否定的。因为当中间用户插入一条记录时，系统会自动清空跟这个表相关的所有缓存记录。当第二次查询时，缓存中已经没有这张表对应的缓存信息。此时就需要重新解析并查询。 四是需要注意，默认字符集对缓存命中率的影响。通常情况下，如果客户端与服务器之间所采用的默认字符集不同，则即使查询语句相同、在两次查询之间记录与表结构也没有被更改，系统仍然认为是不同的查询。对于这一点需要特别的注意，大家比较容易忽视。 "},"you-hua-qi.html":{"url":"you-hua-qi.html","title":"优化器","keywords":"","body":"MySQL会解析查询，并创建内部数据结构（解析树），然后对其进行各种优化，包括重写查询、决定表的读取顺序，以及选择合适的索引等。 用户可以通过特殊的关键字提示（hint）优化器，影响它的决策过程。也可以请求优化器解释（explain）优化过程的各个因素，使用户可以知道服务器是如何进行优化决策的，并提供一个参考基准，便于用户重构查询和schema、修改相关配置，使应用尽可能高效运行 优化器并不关心表使用的是什么存储引擎，但存储引擎对于优化查询是有影响的。优化器会请求存储引擎提供容量或某个具体操作的开销信息，以及表数据的统计信息等。 例如，某些存储引擎的某种索引，可能对一些特定的查询有优化。关于索引与schema的优化， "},"cun-chu-yin-qing.html":{"url":"cun-chu-yin-qing.html","title":"存储引擎","keywords":"","body":""},"innodbwen-jian.html":{"url":"innodbwen-jian.html","title":"InnoDB","keywords":"","body":" 表的定义 在InnoDB引擎中表是根据主键顺序存放的，所以创建一张表时主键是必须存在的且表是根据主键被逻辑存放在一个空间内，这种依据主键存储方式称为索引组织表 InnoDB引擎会按照如下三种方式其中一种来创建主键： 显示定义主键 PRIMARY KEY (id) 判断表中是否有非NULL的唯一索引，如果有该列即为主键。有多个非NULL的唯一索引列默认根据定义最早为主键 `id` int(10) unsigned NOT NULL, UNIQUE KEY `id_UNIQUE` (`id`) 如果不符号上述条件，引擎自动创建一个6字节大小的指针。 CREATE TABLE `test`.`A` ( `a` INT UNSIGNED NOT NULL, `a_v` VARCHAR(45) NULL, UNIQUE INDEX `idA_UNIQUE` (`a` ASC) ); 逻辑数据结构图： 空间的物理存放 在默认配置下会有一个名为ibdata1的文件来存放所有表的数据，该文件就是表空间文件（ tablespace file）也称为共享表空间，但是可以修改默认参数来为每一张创建独立的表空间 同时也可以修改存放路径。 配置文件中设置datadir=xxx 指定数据存放路径 可以通过 innodb data file_ path 设置多个文件组成一个表空间 (路径共享datadir)如下： innodb_data_file_ path=ibdata1:2000M;ibdata2: 2000M:autoextend 段 数据段中存放的是B+数索引，这种索引在叶子节点根据ID顺序存放了整行数据也称为聚集索引，故innodb表称为索引组织表的原因同时也说明为什么innodb必须参加主键的原因。 索引段 成为非聚集索引也称辅助索引，叶子节点并不包含行记录的全部数据。叶子节点除了包含键值以外，每个叶子节点中的索引行中还包含了一个书签（bookmark）。该书签用来告诉InnoDB存储引擎哪里可以找到与索引相对应的行数据。 回滚段 区 区是由连续页组成的空间，在任何情况下每个区的大小都为1MB。为了保证区中页的连续性，InnoDB存储引擎一次从磁盘申请4～5个区 页 同大多数数据库一样，InnoDB有页（Page）的概念（也可以称为块），页是InnoDB磁盘管理的最小单位。 行 InnoDB存储引擎是面向列的（row-oriented），数据是按行进行存放的 B+树索引本身并不能找到具体的一条记录，能找到只是该记录所在的页。数据库把页载入到内存，然后通过PageDirectory再进行二叉查找。只不过二叉查找的时间复杂度很低，同时在内存中的查找很快，因此通常忽略这部分查找所用的时间 "},"innodbwen-jian/suo-yin-zu-zhi-biao.html":{"url":"innodbwen-jian/suo-yin-zu-zhi-biao.html","title":"索引组织表 ","keywords":"","body":"在InnoDB存储引擎中，表都是根据主键顺序组织存放的，这种存储方式的表称为索引组织表（indexorganizedtable）。在InnoDB存储引擎表中，每张表都有个主键（PrimaryKey），如果在创建表时没有显式地定义主键，则InnoDB存储引擎会按如下方式选择或创建主键： 首先判断表中是否有非空的唯一索引（UniqueNOTNULL），如果有，则该列即为主键。 如果不符合上述条件，InnoDB存储引擎自动创建一个6字节大小的指针。 从InnoDB存储引擎的逻辑存储结构看，所有数据都被逻辑地存放在一个空间中，称之为表空间（tablespace）。表空间又由段（segment）、区（extent）、页（page）组成。页在一些文档中有时也称为块（block），InnoDB存储引擎的逻辑存储结构大致 "},"innodbwen-jian/suo-yin-zu-zhi-biao/duan.html":{"url":"innodbwen-jian/suo-yin-zu-zhi-biao/duan.html","title":"段","keywords":"","body":""},"innodbwen-jian/biao-kong-jian/ye.html":{"url":"innodbwen-jian/biao-kong-jian/ye.html","title":"数据页","keywords":"","body":""},"innodbwen-jian/biao-kong-jian.html":{"url":"innodbwen-jian/biao-kong-jian.html","title":"表空间","keywords":"","body":"InnoDB采用将存储的数据按表空间（ tablespace）进行存放的设计。 在默认配置下会有一个初始大小为10MB， 名为ibdata1 的文件。 该文件就是默认的表空间文件（ tablespace file）所有基于InnoDB 存储引擎的表的数据都会记录到该共享表空间中 （1）默认表空间是存放在一个文件中的 配置文件中设置datadir=xxx 指定数据存放路径, （2）可以通过 innodb data file_ path 设置多个文件组成一个表空间 (路径共享datadir) #设置表空间 innodb_data_file_path=ibdata1:2000M;ibdata2:2000M:autoextend #这里将/db/ibdata1 和/dr2/db/ibdata2 两个文件用来组成表空间。 #表示文件idbdata1的大小为2000MB，文件ibdata2的大小为2000MB,如果用完了这2000MB该文件可以自动增长（ autoextend）。 （3）如果设置了innodbfile_per_table=NO 那么每个表都有独立表空间文件,但是这些单独的表空间只存储 数据，索引，缓冲信息，其余还是存放在innodb data file path 设置的表空间里，或者默认的表空间 表ibdata：新建表 ibdata在datadir 数据文件 所对应的目下 会创建两个文件 分别如下： ibdata.frm 表结构，和视图结构 ibdata.ibd 表空间文件 数据，和索引 "},"innodbwen-jian/duo-ban-ben-kong-zhi.html":{"url":"innodbwen-jian/duo-ban-ben-kong-zhi.html","title":"多版本控制","keywords":"","body":"MySQL的大多数事务型存储引擎实现的都不是简单的行级锁。基于提升并发性能的考虑一般都同时实现了多版本并发控制（MVCC）。可以认为MVCC是行级锁的一个变种，但是它在很多情况下避免了加锁操作，因此开销更低。虽然实现机制有所不同，但大都实现了非阻塞的读操作，写操作也只锁定必要的行。 MVCC的实现，是通过保存数据在某个时间点的快照来实现的，也就是说，不管需要执行多长时间，每个事务看到的数据都是一致的。根据事务开始的时间不同，每个事务对同一张表，同一时刻看到的数据可能是不一样的。不同存储引擎的MVCC实现是不同的，典型的有乐观（optimistic）并发控制和悲观（pessimistic）并发控制 InnoDB下的MVCC实现 通过在每行记录后面保存两个隐藏的列来实现的，一个保存了行的创建时间（系统版本号），一个保存行的过期时间（或删除时间）。 每开始一个新的事务，系统版本号都会自动递增 事务开始时刻的系统版本号会作为事务的版本号，用来和查询到的每行记录的版本号进行比较。 保存这两个额外系统版本号，使大多数读操作都可以不用加锁。这样设计使得读数据操作很简单，性能很好，并且也能保证只会读取到符合标准的行。不足之处是每行记录都需要额外的存储空间，需要做更多的行检查工作，以及一些额外的维护工作。 "},"innodbwen-jian/duo-ban-ben-kong-zhi/shi-wu-ge-li-ji-bie-shi-xian.html":{"url":"innodbwen-jian/duo-ban-ben-kong-zhi/shi-wu-ge-li-ji-bie-shi-xian.html","title":"事务隔离级别表现","keywords":"","body":"REPEATABLE READ 和 READ COMMITTED 下的MVCC操作 select操作下InnoDB会根据以下两个条件检查每行记录 InnoDB只查找版本早于当前事务版本的数据行（也就是，行的系统版本号小于或等于事务的系统版本号），这样可以确保事务读取的行，要么是在事务开始前已经存在的，要么是事务自身插入或者修改过的。 行的删除版本(隐藏列过期版本)要么未定义，要么大于当前事务版本号，确保事务读取到的行，在事务开始之前未被删除。 只有符合上述两个条件的记录，才能返回作为查询结果。 模拟多版本实现过程 CREATE TABLE `tcss`.`snapshot` ( `id` INT NOT NULL, `createVersion` INT NOT NULL, `deleVersion` INT NOT NULL, `tableName` VARCHAR(45) NULL, PRIMARY KEY (`id`)); INSERT 操作下 InnoDB为新插入的每一行保存当前系统版本号作为行版本号 DELETE 操作下 InnoDB为删除的每一行保存当前系统版本号作为行删除标识 UPDATE 操作下 InnoDB为插入一行新记录，保存当前系统版本号作为行版本号，同时保存当前系统版本号到原来的行作为行删除标识。保存这两个额外系统版本号，使大多数读操作都可以不用加锁。这样设计使得读数据操作很简单，性能很好，并且也能保证只会读取到符合标准的行。不足之处是每行记录都需要额外的存储空间，需要做更多的行检查工作，以及一些额外的维护工作。 MVCC只在REPEATABLE READ和READ COMMITTED两个隔离级别下工作，其他两个隔离级别都和MVCC不兼容因为READ UNCOMMITTED总是读取最新的数据行，而不是符合当前事务版本的数据行。而SERIALIZABLE则会对所有读取的行都加锁。 "},"memory.html":{"url":"memory.html","title":"Memory","keywords":"","body":""},"ri-zhi-lei-xing.html":{"url":"ri-zhi-lei-xing.html","title":"日志类型","keywords":"","body":"主要包含：错误日志、查询日志、慢查询日志、事务日志、二进制日志； 日志是mysql数据库的重要组成部分。日志文件中记录着mysql数据库运行期间发生的变化；也就是说用来记录mysql数据库的客户端连接状况、SQL语句的执行情况和错误信息等。当数据库遭到意外的损坏时，可以通过日志查看文件出错的原因，并且可以通过日志文件进行数据恢复。 一般而言，日志级别的定义没有会话变量 都只是在全局级别下进行定义,查看日志配置信息: SHOW GLOBAL VARIABLES LIKE '%log%' 错误日志 在mysql数据库中，错误日志功能是默认开启的。并且，错误日志无法被禁止。默认情况下，错误日志存储在mysql数据库的数据文件中。错误日志文件通常的名称为hostname.err。其中，hostname表示服务器主机名,默认情况下错误日志大概记录以下几个方面的信息： 服务器启动和关闭过程中的信息（未必是错误信息，如mysql如何启动InnoDB的表空间文件的、如何初始化自己的存储引擎的等等） 服务器运行过程中的错误信息、事件调度器运行一个事件时产生的信息、在从服务器上启动服务器进程时产生的信息 log_error = /var/log/mysql/error.log. 在mysql.cnf文件中设置。 log-error 其中log-err是定义是否启用错误日志的功能和错误日志的存储位置。 log-warnings: 是定义是否将警告信息也定义至错误日志中。 其中，log_error可以直接定义为文件路径，也可以为ON|OFF；log_warings只能使用1|0来定义开关启动 查询日志 默认情况下查询日志是关闭的。由于查询日志会记录用户的所有操作，其中还包含增删查改等信息 general_log_file= /var/log/mysql/general.log general_log= 1 | 0 开启|关闭 在并发操作大的环境下会产生大量的信息从而导致不必要的磁盘IO，会影响mysql的性能的。如若不是为了调试数据库的目的建议不要开启查询日志 慢查询日志 慢查询日志是用来记录执行时间超过指定时间的查询语句,通过慢查询日志，可以查找出哪些查询语句的执行效率很低,以便进行优化。 #开启 slow_query_log=1 | 0 #慢查询日志文件路径 slow-query-log-file=/var/log/mysql/query.log #超过多少秒的查询就写入日 0 表示全部 long_query_time=1 | 0 一般建议开启，它对服务器性能的影响微乎其微，但是可以记录mysql服务器上执行了很长时间的查询语句。可以帮助我们定位性能问题的 事务日志（重做日志）开启事务记录所有操作，可以回滚（InnoDB特有的日志）可以帮助提高事务的效率。 使用事务日志，存储引擎在修改表的数据时只需要修改其内存拷贝，再把改修改行为记录到持久在硬盘上的事务日志中，而不用每次都将修改的数据本身持久到磁盘。事务日志采用追加的方式，因此写日志的操作是磁盘上一小块区域内的顺序I/O，而不像随机I/O需要在磁盘的多个地方移动磁头，所以采用事务日志的方式相对来说要快得多。事务日志持久以后，内存中被修改的数据在后台可以慢慢的刷回到磁盘。目前大多数的存储引擎都是这样实现的，我们通常称之为预写式日志，修改数据需要写两次磁盘。 redo log 用来保证事务的持久性 undo log 用来帮助事务回滚及MVCC的功能 innodb_flush_log_at_trx_commit 该参数的默认值为1， 表示事务提交时必须调用一次fsync操作。 还可以设置该参数的值为0和2。 0表示事务提交时不进行写入重做日志操作， 2表示事务提交时将重做日志写入重做日志 文件， 但仅写入文件系统的缓存中， 不进行fsync操作。 二进制日志—事务提交记录(表结构以及约束) "},"ri-zhi-lei-xing/er-jin-zhi-ri-zhi.html":{"url":"ri-zhi-lei-xing/er-jin-zhi-ri-zhi.html","title":"二进制日志","keywords":"","body":"二进制日志也叫作变更日志，主要用于记录修改数据或有可能引起数据改变的mysql语句，默认是关闭的 每次重启mysql服务或运行mysql flush logs ; 都会生成一个新的二进制日志文件，这些日志文件的数量会不断地递增。 filename.index 文件中存储所有二进制日志文件的清单又称为二进制文件的索引。 #服务ID server-id= 1 #目录 log_bin= /var/log/mysql/mysql-bin.log #日志文件大小 max_binlog_size = 100M #表示二进制过期时间为10天，每次进行flush logs 会删除过期时间 或者重新启动，或者超过max_binlog_size值 expire_logs_days = 10 #持久化设置 sync_binlog=0 #指定数据库生成二进制日志 binlog_do_db=adb binlog_do_db=bdb #忽略 binlog-ignore-db=information_schema #查询所有二进制文件。 show binary logs; #查看当前二进制文件中发生的事情。 show binlog events in 'mysql-bin.00000x’; #通过恢复到设定的状态。 mysqlbinlog mysql-bin.000002 --start-position=796 --stop-position=985 | mysql -uroot -p sync_binlog 在MySQL中系统默认的设置是sync_binlog=0，也就是不做任何强制性的磁盘刷新指令，这时候的性能是最好的，但是风险也是最大的 sync_binlog=n 当每进行n次事务提交之后，MySQL将进行一次fsync之类的磁盘同步指令来将binlog_cache中的数据强制写入磁盘。 sync_binlog=1 立刻同步 对于高并发事务的系统来说，“sync_binlog”设置为0和设置为1的系统写入性能差距可能高达5倍甚至更多 指定或者忽略生成二进制日志 binlog_do_db 指定生成二进制日志的数据库 binlog-ignore-db 忽略 主存复制配置设置相同 "},"ri-zhi-lei-xing/man-cha-xun-ri-zhi.html":{"url":"ri-zhi-lei-xing/man-cha-xun-ri-zhi.html","title":"慢查询日志","keywords":"","body":"在优化查询中确定查询只返回需要的数据以后，检查返回结果是否扫描过的数据。 如何衡量查询开销 如下三个指标 响应时间 扫描的行数 返回的行数 这 三个指标都会记录到MySQL的慢日志中。MySQL有好几种访问方式可以查找并返回一行结果。 有些访问方式可能需要扫描很多行才能返回一行结果， 也有些访问方式可能无须扫描就能返回结果。 如 查询没有办法找到合适的访问类型， 那么解决的最 办法通常就是增加一个合适的索引， "},"ri-zhi-lei-xing/man-cha-xun-ri-zhi/pt-query-digest.html":{"url":"ri-zhi-lei-xing/man-cha-xun-ri-zhi/pt-query-digest.html","title":"pt-query-digest","keywords":"","body":"pt-query-digest是用于分析mysql慢查询的一个工具。它可以分析binlog、General log、slowlog，也可以通过SHOWPROCESSLIST或者通过tcpdump抓取的MySQL协议数据来进行分析。可以把分析结果输出到文件中，分析过程是先对查询语句的条件进行参数化，然后对参数化以后的查询进行分组统计，统计出各查询的执行时间、次数、占比等，可以借助分析结果找出问题进行优化 #安装 sudo apt install percona-toolkit #慢查询日志分析统计 pt-query-digest slow.log # 5.6s user time, 40ms system time, 37.44M rss, 108.07M vsz # Current date: Tue Sep 24 17:47:53 2019 # Hostname: baihui-desktop # Files: slow.log # Overall: 8.63k total, 82 unique, 12.78 QPS, 0.01x concurrency __________ # Time range: 2019-09-24T09:36:36 to 2019-09-24T09:47:51 # Attribute total min max avg 95% stddev median # ============ ======= ======= ======= ======= ======= ======= ======= # Exec time 8s 3us 717ms 875us 881us 9ms 84us # Lock time 566ms 0 99ms 65us 194us 2ms 0 # Rows sent 4.51k 0 500 0.54 0.99 9.55 0 # Rows examine 35.84k 0 23.99k 4.25 0.99 261.59 0 # Query size 1.38M 6 4.76k 167.16 621.67 168.68 102.22 # Profile # Rank Query ID Response time Calls R/Call V/M Item "},"biao-fen-qu.html":{"url":"biao-fen-qu.html","title":"分区","keywords":"","body":""},"biao-fen-qu/shui-ping-fen-qu.html":{"url":"biao-fen-qu/shui-ping-fen-qu.html","title":"水平分区","keywords":"","body":""},"biao-fen-qu/shui-ping-fen-qu/mysql.html":{"url":"biao-fen-qu/shui-ping-fen-qu/mysql.html","title":"mysql","keywords":"","body":"分区表 分区功能是在存储引擎层完成的,因此不只有innodb支持,分区的过程是将一个表或者索引分成多个更小，更可以管理的部分。对应用来说只有一个表，但是物理上是多个，mysql数据库支持的类型为水平分区，不支持垂直分区，对分区中的NULL值也支持但是各个分区类型处理不同 分区的字段必须是要包含在主键当中。这时候分区的字段要么是主键，要么把分区字段加入到主键中，从而形成复合主键 --创建分区时，当前列不在主键索引中错误 PRIMARY KEY must include all columns in the table's partitioning function。 Mysql分区类型有如下几种： RANGE（范围分区） 使用VALUES LESS THAN（xx）语句（比xx小或者等于它的数据在当前分区），范围分区是基于一个给定数值匹配一个离散集合中的某个值的范围匹配 如下 【10,20,30,40 max】是离散集合 -- 如按照年份将表中所有行分成到5个区不同的区域： CREATE TABLE user_age ( name varchar(30) default NULL, age int default NULL ) PARTITION BY RANGE (year(age)) ( #小于10在p0分区 PARTITION p0 VALUES LESS THAN (10) , #大于10 小于20 PARTITION p1 VALUES LESS THAN (20) , PARTITION p2 VALUES LESS THAN (30) , PARTITION p3 VALUES LESS THAN (40) , #大于40 PARTITION p5 VALUES LESS THAN MAXVALUE ); -- p2 区 explain partitions SELECT * FROM incar.user_age where age = 21.5; '1','SIMPLE','user_age','p2','ALL',NULL,NULL,NULL,NULL,'2','50.00','Using where' LIST(列表分区) 类似于按RANGE分区，区别在于LIST分区是基于列值匹配一个离散值集合中的某个值来进行选择固定值精确匹配（VALUES IN）。 insert的列值必须是在分区集合致中定义否者无法添加 CREATE TABLE t( a INT, b INT) ENGINE= INNODB PARTITION BY LIST(b) ( PARTITION p0 VALUES IN( 1, 3, 5, 7, 9), PARTITION p1 VALUES IN( 0, 2, 4, 6, 8) ) #分区命中是p1 explain partitions select * from t where b = 2 #插入一个未在集合中定义的值 insert into t (a,b) values(3,10) ; 错误： insert into t (a,b) values(3,10) Error Code: 1526. Table has no partition for value 10 0.0017 sec HASH分区 基于用户定义的表达式的返回值来进行选择的分区，该表达式使用将要插入到表中的这些行的列值进行计算。这个函数可以包含MySQL中有效的、产生非负整数值的任何表达式 #b使用hash分成4个区 CREATE TABLE t_hash( a INT, b DATETIME ) ENGINE= InnoDB PARTITION BY HASH(YEAR(b)) PARTITIONS 4; LINEAR HASH 分区的优点在于，增加、删除、合并和拆分分区将变得更加快捷 KEY分区 KEY分区和HASH分区相似，但是KEY分区支持除text和BLOB之外的所有数据类型的分区，而HASH分区只支持数字分区，KEY分区不允许使用用户自定义的表达式进行分区，KEY分区使用系统提供的HASH函数进行分区 NULL 在分区中MYSQL总是视NULL值视小于任何的一个非NULL值，这和ORDER BY操作是一样 * RANGE分区 如果向分区列插入了NULL值，则会将该值放入分区值是最小的分区上。 * LIST 分区 必须在分区值中添加 NULl 无论创建何种分区，如果表中有唯一索引或者主键，分区列必须包含其中列,如果表中没指定唯一和主键那么就选择其它列。 查询分区表信息的几种方法 //查看创建分区表的create语句以及分区信息 show create table tx; //查看那些表是分区表 show table status ; //查看information_schema.partitions表 select partition_name,table_rows from information_schema.partitions where table_name='tx'; //通过此语句来显示扫描哪些分区，及他们是如何使用的 explain partitions select 给表创建分区 -- 列表分区 alter table tx partition by list(deptno)(partition p1 values in (10),partition p2 values in (20),partition p3 values in (30)); -- HASH分区 alter table tx PARTITION BY HASH(YEAR(b)) PARTITIONS 4 添加分区 如果添加的分区比第一个分区小，就需要重新组织分区 ALTER TABLE members ADD PARTITION (PARTITION p3 VALUES LESS THAN (2000)); 合并分区 ALTER TABLE members REORGANIZE PARTITION s0,s1 INTO ( PARTITION p0 VALUES LESS THAN (1970)); 将s0 和s1合并到p0中 hash和key 合并分区 ALTER TABLE clients COALESCE PARTITION 4;将6个分区合并4个分区 分区裁剪 分区裁剪 是自动做的，选择分区可以明确的指定要操作哪些分区 版本5.1以上 select * from `employees` PARTITION(p0) where id = 1 ; SELECT store_id, COUNT(department_id) AS c FROM employees PARTITION (p1,p2,p3) GROUP BY store_id HAVING c > 4;–这些操作都是可以的 DELETE FROM employees PARTITION (p0, p1) WHERE fname LIKE ‘j%’; UPDATE employees PARTITION (p0) SET store_id = 2 WHERE fname = ‘Jill’; 删除分区几种方法 alter table tx drop partition p1 ; 这种方法回删除数据， 不可以用作删除hash或者key分区。 alter table tx remove partitioning ; 删除全部分区 不会丢失数据,可以用作key和hash 使用remove移除分区是仅仅移除分区的定义，并不会删除数据和drop PARTITION不一样，后者会连同数据一起删除 key分区管理和hash分区管理是一样的，只能删除和增加分区 删除2个分区 ALTER TABLE tb_key COALESCE PARTITION 2 ; 增加三个分区 ALTER TABLE tb_key add PARTITION partitions 3 ; 启用分区可以带来一定性能提升如下 与单个磁盘或文件系统分区相比，可以存储更多的数据 对于那些已经失去保存意义的数据，通常可以通过删除与那些数据有关的分区，很容易地删除那些数据。相反地，在某些情况下，添加新数据的过程又可以通过为那些新数据专门增加一个新的分区，来很方便地实现。通常和分区有关的其他优点包括下面列出的这些。MySQL分区中的这些功能目前还没有实现，但是在我们的优先级列表中，具有高的优先级；我们希望在5.1的生产版本中，能包括这些功能。 一些查询可以得到极大的优化，这主要是借助于满足一个给定WHERE语句的数据可以只保存在一个或多个分区内，这样在查找时就不用查找其他剩余的分区。因为分区可以在创建了分区表后进行修改，所以在第一次配置分区方案时还不曾这么做时，可以重新组织数据，来提高那些常用查询的效率。 涉及到例如SUM()和COUNT()这样聚合函数的查询，可以很容易地进行并行处理。这种查询的一个简单例子如 “SELECT salesperson_id, COUNT (orders) as order_total FROM sales GROUP BY salesperson_id；”。通过“并行”，这意味着该查询可以在每个分区上同时进行，最终结果只需通过总计所有分区得到的结果。 通过跨多个磁盘来分散数据查询，来获得更大的查询吞吐量。 ·在MYSQL5.1 版本中分区表达式的结果必须是整数，在MYSQL5.5分区表达式可以使用列 ·如果分区字段中有主键或者是唯一索引列，则所有的主键或者是唯一索引列必须全部包含进来 ·分区表无法使用外键 ·对于同一个表的各个分区表必须使用相同的存储引擎 ·分区函数有限制，只可以是MySQL 中有效的任何函数或其他表达式，且它们返回一个既非常数、也非随机数的整数 使用分区表的“陷阱”NULL值 MySQL 中的分区在禁止空值（NULL）上没有进行处理，无论它是一个列值还是一个用户定义表达式的值。一般而言，对于NULL,或者是当表达式接收非法值时（e.g. YEAR(‘asdf-12-12’)）返回的结果都是NULL,在这种情况下MySQL 把NULL视为0,如果大量的记录存在这种情况，最终会导致大量的记录都集中在一个分区中，也也就违背了分区的初衷。 如果你希望回避这种做法，你应该在设计表时不允许空值；最可能的方法是，通过声明列“NOT NULL”来实现这一点。 分区列和索引列不匹配 如果定义的索引列和分区列不匹配，则会导致查询无法进行分区过滤。例如在列a上定义分区，在列b上定义索引，因为每个分区都有独立的索引，所以扫描索引时需要扫描每个分区。 优化查询语句 对于分区表的访问，最重要的一点是要在where条件中包含分区列，即使看起来是多余的，只有这样才能过滤不需要的分区，否则会访问所以的分区表。 表的优化 当数据库数据量涨到一定数量时，性能就成为我们不能不关注的问题，如何优化呢？ 常用的方式不外乎那么几种： 分表 ，即把一个很大的表达数据分到几个表中，这样每个表数据都不多。 优点：提高并发量，减小锁的粒度 缺点：代码维护成本高，相关sql都需要改动 分区 ，所有的数据还在一个表中，但物理存储数据根据一定的规则存放在不同的文件中，文件也可以放到另外磁盘上 优点：代码维护量小，基本不用改动，提高IO吞吐量 缺点：表的并发程度没有增加 拆分业务 ，这个本质还是分表 优点：长期支持更好 缺点：代码逻辑重构，工作量很大 "},"biao-fen-qu/chui-zhi-fen-qu.html":{"url":"biao-fen-qu/chui-zhi-fen-qu.html","title":"垂直分区","keywords":"","body":""},"fu-zhi.html":{"url":"fu-zhi.html","title":"复制","keywords":"","body":" 开启主库二进制日志 设置server-id 编号 从库开启二进制日志， 设置server-id编号 设置master地址 change master to master_host='192.168.1.12', master_port=3306, master_user='root', master_password='root', master_log_file='mysql-bin.000001', master_log_pos=0; //偏移量 指定字节数。0表示：从 二进制日志 0位置开始， （master 上的Position 值是纪录写到多了日志） 启动/停止复制 start/stop slave slave首次同步需要建立表结构,才数据同步 下面六项需要在slave上设置： Replicate_Do_DB:设定需要复制的数据库,多个DB用逗号分隔 Replicate_Ignore_DB:设定可以忽略的数据库.—中继站忽略 Replicate_Do_Table:设定需要复制的Table Replicate_Ignore_Table:设定可以忽略的Table Replicate_Wild_Do_Table:功能同Replicate_Do_Table,但可以带通配符来进行设置。 Replicate_Wild_Ignore_Table:功能同Replicate_Do_Table,功能同Replicate_Ignore_Table,可以带通配符。 优点是在slave端设置复制过滤机制,可以保证不会出现因为默认的数据库问题而造成Slave和Master数据不一致或复制出错的问题. 缺点是性能方面比在Master端差一些.原因在于:不管是否须要复制,事件都会被IO线程读取到Slave端,这样不仅增加了网络IO量,也给Slave端的IO线程增加了Relay Log的写入量。 在实际的生产应用中发现，在mysql5.0以前的版本，mysql的这个过滤设置几乎是形同虚设，不起作用：不管你在主库或是从库上设置了忽略某个数据库或是表，他依然会进行同步，所以在做5.0以前版本的主从同步时，一定保持主从数据库的一致性，主上有的库或是表从上一定要有，否则在同步的过程会出错 Slave尽量使用read_only，它防止改变数据(除了特殊的线程)。但是，read_only并是很实用，特别是那些需要在slave上创建表的应用 MySQL主从复制几个重要的启动选项 log-slave-updates这个参数用来配置从服务器的更新是否写入二进制日志，这个选项默认是不打开的，但是，如果这个从服务器B是服务器A的从服务器，同时还作为服务器C的主服务器，那么就需要开发这个选项，这样它的从服务器C才能获得它的二进制日志进行同步操作 master-connect-retry这个参数是用来设置在和主服务器连接丢失的时候，重试的时间间隔，默认是60秒 read-only是用来限制普通用户对从数据库的更新操作，以确保从数据库的安全性，不过如果是超级用户依然可以对从数据库进行更新操作 slave-skip-errors 在复制过程中，由于各种的原因，从服务器可能会遇到执行BINLOG中的SQL出错的情况，在默认情况下，服务器会停止复制进程，不再进行同步，等到用户自行来处理。　　Slave-skip-errors的作用就是用来定义复制过程中从服务器可以自动跳过的错误号，当复制过程中遇到定义的错误号，就可以自动跳过，直接执行后面的SQL语句。　　--slave-skip-errors=[err1,err2,…….|ALL]　　但必须注意的是，启动这个参数，如果处理不当，很可能造成主从数据库的数据不同步，在应用中需要根据实际情况，如果对数据完整性要求不是很严格，那么这个选项确实可以减轻维护的成本 工作原理分为三步 主服务(master)把数据更改且已经提交事务的数据记录到二进制日志（binlog）中 主服务(master)发送二进制内容根据当前position偏移量， slave通过IO线程将二进制日志写入到relay_log_file中并且记录read_master_log_pos偏移量 slave存服务器sql线程读relay_log_file文件并且记录偏移量relay_log_pos sql线程会重新写入到中继日志文件relay_master_log_file中 最后执行relay_master_log_file文件中的内容执行SQL记录偏移量exe_master_log_pos 在设置主服务器时master_log_pos = xx 和master_log_file = xx master_log_file = XX 表示当前从服务器需要复制的主服务器的二进制文件名称（如果当前二进制文件未删除）会根据实际情况来选择复制那个二进制文件(slave 服务器文件relay_log_file 中未记录xx的文件记录) stop slave ; change master to master_log_file='mysql-bin.000009' ,master_log_pos=0 ; 当前master 二进制已经发送到mysql-bin.000013 了但是可以要求master 对这台slave 从 mysql-bin.000009文件开始 （即使当前slave服务器直接设置mysql-bin.000013 master会根据当前slave relay日志来确定是否发送最早的二进制文件） master_log_file = xx 二进制日子文件的发生的偏移量地址 /var/lib/mysql/master.info 可以看到配置好的信息 master_log_file=“xxx” 如在master服务器上的二进制文件以过期被删除,将无法复制 change master to master_host=' ‘; 即可成功删除同步用户信息 set global sql_slave_skip_counter=N 中的N是指跳过N个event最好记的是N被设置为1时，效果跳过下一个事务。跳过第N个event后，位置若刚好落在一个事务内部，则会跳过这整个事务 一个insert/update/delete不一定只对应一个event，由引擎和日志格式决定。 SQL 线程错误一般是因为修改表结果或者主键冲突 约束 数据的一致性问题，对数据本身的修改，不涉及到一致性完整性都是可以的 MySQL同步故障：\" Slave_SQL_Running:No\" 解决办法 mysql> show slave status\\G.......Relay_Log_File: localhost-relay-bin.000535Relay_Log_Pos: 21795072Relay_Master_Log_File: localhost-bin.000094Slave_IO_Running: YesSlave_SQL_Running: NoReplicate_Do_DB:Replicate_Ignore_DB:解决办法一、Slave_SQL_Running: No1.程序可能在slave 上进行了写操作2.也可能是slave 机器重起后，事务回滚造成的.一般是事务回滚造成的：解决办法：mysql> stop slave ;mysql> set GLOBAL SQL_SLAVE_SKIP_COUNTER=1;mysql> start slave ;解决办法二、首先停掉Slave 服务：slave stop到主服务器上查看主机状态：记录File 和Position 对应的值进入mastermysql> show master status;+----------------------+----------+--------------+------------------+| File | Position | Binlog_Do_DB | Binlog_Ignore_DB |+----------------------+----------+--------------+------------------+| localhost-bin.000094 | 33622483 | | |+----------------------+----------+--------------+------------------+1 row in set (0.00 sec)然后到slave 服务器上执行手动同步：mysql> change master to 查看mysql主从配置的状态及修正 slave不启动问题 1、查看master的状态show master status; //Position不应该为0show processlist;//state状态应该为Has sent all binlog to slave; waiting for binlog to be updated2、查看slave状态show slave status;//Slave_IO_Running 与 Slave_SQL_Running 状态都要为Yesshow processlist;//应该有两行state值为：Has read all relay log; waiting for the slave I/O thread to update itWaiting for master to send event 3、错误日志MySQL安装目录 /usr/local/mysqlMySQL日志目录 /usr/local/mysql/data/ 形如，Hostname.err 4、Change master to如果从库的Slave未启动，Slave_IO_Running为NO。可能是主库是的master的信息有变化，查看主库show master status;记录下File,Position字段，假设为‘mysql-bin.000004’,98;在从库执行： mysql>stop slave; mysql>change master to master_log_file='mysql-bin.000004',master_log_pos=98; mysql>start slave;5、SET global sql_slave_skip_counter=n;如果从库的slave_sql_running为NO。Err文件中记录：Slave:Error \"Duplicate entry '1' for key 1\" on query.....可能是master未向slave同步成功，但slave中已经有了记录。造成的冲突可以在从库上执行set global sql_slave_skip_counter=n;跳过几步。再restart slave就可以了。 6、同步错误处理发现mysql slave服务器经常因为一些特殊字符或者符号产生的更新语句报错，整个同步也会因此而卡在那，最初的办法只是手动去出错的机器执行下面三条SQL语句，跳过错误即可。 mysql>slave stop; mysql>set GLOBAL SQL_SLAVE_SKIP_COUNTER=1; mysql>slave start;PS：本人多次遇到从数据库的同步进程自动停掉的问题，有时简单通过slave stop,slave start即可解决。有时slave start启动后又会自动停掉，这时使用 change master重设主数据库信息的方式解决了问题。 slave_IO_Running：连接到主库，并读取主库的日志到本地，生成本地日志文件slave_SQL_Running:读取本地日志文件，并执行日志里的SQL命令。 "},"zhu-cong-tong-bu-yan-chi.html":{"url":"zhu-cong-tong-bu-yan-chi.html","title":"主从同步延迟","keywords":"","body":"MySQL的主从同步是一个很成熟的架构，优点为： ①在从服务器可以执行查询工作(即我们常说的读功能)，降低主服务器压力; ②在从主服务器进行备份，避免备份期间影响主服务器服务; ③当主服务器出现问题时，可以切换到从服务器。 相信大家对于这些好处已经非常了解了，在项目的部署中也采用这种方案。但是MySQL的主从同步一直有从库延迟的问题，那么为什么会有这种问题。这种问题如何解决呢？ MySQL数据库主从同步延迟原理。 MySQL数据库主从同步延迟是怎么产生的。 MySQL数据库主从同步延迟解决方案。 MySQL数据库主从同步延迟原理。 答：谈到MySQL数据库主从同步延迟原理，得从mysql的数据库主从复制原理说起，mysql的主从复制都是单线程的操作，主库对所有DDL和 DML产生binlog，binlog是顺序写，所以效率很高，slave的Slave_IO_Running线程到主库取日志，效率很比较高，下一步， 问题来了，slave的Slave_SQL_Running线程将主库的DDL和DML操作在slave实施。DML和DDL的IO操作是随即的，不是顺 序的，成本高很多，还可能可slave上的其他查询产生lock争用，由于Slave_SQL_Running也是单线程的，所以一个DDL卡主了，需要 执行10分钟，那么所有之后的DDL会等待这个DDL执行完才会继续执行，这就导致了延时。有朋友会问：“主库上那个相同的DDL也需要执行10分，为什 么slave会延时？”，答案是master可以并发，Slave_SQL_Running线程却不可以。 MySQL数据库主从同步延迟是怎么产生的。 答：当主库的TPS并发较高时，产生的DDL数量超过slave一个sql线程所能承受的范围，那么延时就产生了，当然还有就是可能与slave的大型query语句产生了锁等待。 MySQL数据库主从同步延迟解决方案 答：最简单的减少slave同步延时的方案就是在架构上做优化，尽量让主库的DDL快速执行。还有就是主库是写，对数据安全性较高，比如 sync_binlog=1，innodb_flush_log_at_trx_commit = 1 之类的设置，而slave则不需要这么高的数据安全，完全可以讲sync_binlog设置为0或者关闭binlog，innodb_flushlog也 可以设置为0来提高sql的执行效率。另外就是使用比主库更好的硬件设备作为slave。 附加： sync_binlog 配置说明：二进制日志 sync_binlog”：这个参数是对于MySQL系统来说是至关重要的，他不仅影响到Binlog对MySQL所带来的性能损耗，而且还影响到MySQL中数据的完整性。对于“sync_binlog”参数的各种设置的说明如下： sync_binlog=0，当事务提交之后，MySQL不做fsync之类的磁盘同步指令刷新binlog_cache中的信息到磁盘，而让Filesystem自行决定什么时候来做同步，或者cache满了之后才同步到磁盘。 sync_binlog=n，当每进行n次事务提交之后，MySQL将进行一次fsync之类的磁盘同步指令来将binlog_cache中的数据强制写入磁盘。 在MySQL中系统默认的设置是sync_binlog=0，也就是不做任何强制性的磁盘刷新指令，这时候的性能是最好的，但是风险也是最大的。因为一旦系统Crash，在binlog_cache中的所有binlog信息都会被丢失。而当设置为“1”的时候，是最安全但是性能损耗最大的设置。因为当设置为1的时候，即使系统Crash，也最多丢失binlog_cache中未完成的一个事务，对实际数据没有任何实质性影响。 从以往经验和相关测试来看，对于高并发事务的系统来说，“sync_binlog”设置为0和设置为1的系统写入性能差距可能高达5倍甚至更多。 innodb_flush_log_at_trx_commit 配置说明：事务日志 默认值1的意思是每一次事务提交或事务外的指令都需要把日志写入（flush）硬盘，这是很费时的。特别是使用电 池供电缓存（Battery backed up cache）时。设成2对于很多运用，特别是从MyISAM表转过来的是可以的，它的意思是不写入硬盘而是写入系统缓存。日志仍然会每秒flush到硬 盘，所以你一般不会丢失超过1-2秒的更新。设成0会更快一点，但安全方面比较差，即使MySQL挂了也可能会丢失事务的数据。而值2只会在整个操作系统 挂了时才可能丢数据。 mysql-5.6.3已经支持了多线程的主从复制。原理和丁奇的类似，丁奇的是以表做多线程，Oracle使用的是以数据库(schema)为单位做多线程，不同的库可以使用不同的复制线程。 基于局域网的master/slave机制在通常情况下已经可以满足'实时'备份的要求了。如果延迟比较大，就先确认以下几个因素： 网络延迟 master负载 slave负载 一般的做法是，使用多台slave来分摊读请求，再从这些slave中取一台专用的服务器，只作为备份用，不进行其他任何操作，就能相对最大限度地达到'实时'的要求了 slave_net_timeout单位为秒 默认设置为 3600秒 参数含义：当slave从主数据库读取log数据失败后，等待多久重新建立连接并获取数据 master-connect-retry单位为秒 默认设置为 60秒 参数含义：当重新建立主从连接时，如果连接建立失败，间隔多久后重试。 通常配置以上2个参数可以减少网络问题导致的主从数据同步延迟 判断主从延时，通常有两个方法： Seconds_Behind_Master vs 2. mk-heartbeat，下面具体说下两者在实现功能的差别。 可以通过监控show slave status\\G命令输出的Seconds_Behind_Master参数的值来判断，是否有发生主从延时。 其值有这么几种： NULL - 表示io_thread或是sql_thread有任何一个发生故障，也就是该线程的Running状态是No,而非Yes. 0 - 该值为零，是我们极为渴望看到的情况，表示主从复制良好，可以认为lag不存在。 正值 - 表示主从已经出现延时，数字越大表示从库落后主库越多。 负值 - 几乎很少见，只是听一些资深的DBA说见过，其实，这是一个BUG值，该参数是不支持负值的，也就是不应该出现。 Seconds_Behind_Master是通过比较sql_thread执行的event的timestamp和io_thread复制好的 event的timestamp(简写为ts)进行比较，而得到的这么一个差值。我们都知道的relay-log和主库的bin-log里面的内容完全一 样，在记录sql语句的同时会被记录上当时的ts，所以比较参考的值来自于binlog，其实主从没有必要与NTP进行同步，也就是说无需保证主从时钟的 一致。你也会发现，其实比较真正是发生在io_thread与sql_thread之间，而io_thread才真正与主库有关联，于是，问题就出来了， 当主库I/O负载很大或是网络阻塞，io_thread不能及时复制binlog（没有中断，也在复制），而sql_thread一直都能跟上 io_thread的脚本，这时Seconds_Behind_Master的值是0，也就是我们认为的无延时，但是，实际上不是，你懂得。这也就是为什 么大家要批判用这个参数来监控数据库是否发生延时不准的原因，但是这个值并不是总是不准，如果当io_thread与master网络很好的情况下，那么 该值也是很有价值的。（就好比：妈–儿子–媳妇的关系，妈与儿子亲人，媳妇和儿子也亲人，不见得媳妇与妈就很亲。开个玩笑:-）之前，提到 Seconds_Behind_Master这个参数会有负值出现，我们已经知道该值是io_thread的最近跟新的ts与sql_thread执行到 的ts差值，前者始终是大于后者的，唯一的肯能就是某个event的ts发生了错误，比之前的小了，那么当这种情况发生时，负值出现就成为可能。 方法2. mk-heartbeat，Maatkit万能工具包中的一个工具，被认为可以准确判断复制延时的方法。 mk-heartbeat的实现也是借助timestmp的比较实现的，它首先需要保证主从服务器必须要保持一致，通过与相同的一个NTP server同步时钟。它需要在主库上创建一个heartbeat的表，里面至少有id与ts两个字段，id为server_id，ts就是当前的时间戳 now()，该结构也会被复制到从库上，表建好以后，会在主库上以后台进程的模式去执行一行更新操作的命令，定期去向表中的插入数据，这个周期默认为1 秒，同时从库也会在后台执行一个监控命令，与主库保持一致的周期去比较，复制过来记录的ts值与主库上的同一条ts值，差值为0表示无延时，差值越大表示 延时的秒数越多。我们都知道复制是异步的ts不肯完全一致，所以该工具允许半秒的差距，在这之内的差异都可忽略认为无延时。这个工具就是通过实打实的复 制，巧妙的借用timestamp来检查延时，赞一个！ "},"zhu-cong-tong-bu-yan-chi/pei-zhi.html":{"url":"zhu-cong-tong-bu-yan-chi/pei-zhi.html","title":"配置","keywords":"","body":" #配置my.cnf文件 server-id = 2 log_bin = /var/log/mysql/mysql-bin.log expire_logs_days = 10 max_binlog_size = 100M #指定复制的数据库 replicate-do-db=nextcloud #--------------------------------------------- #偏移量 指定字节数。0表示：从 二进制日志 0位置开始，（master 上的Position 值是纪录写到多了日志） change master to master_host='192.168.1.8', master_port=3306, master_user='root', master_password='root', master_log_file='mysql-bin.000001', master_log_pos=0; #启动/复制 start/stop slave; #运行状态 show slave status; salve首次同步是否需要建立表结构或者数据库？ 取决于master上的二进制是否删除或者在master_log_file中指定的二进制文件是否包含创建表以及数据库的记录, 指定或者忽略复制的二进制数据日志在slave下面六项需要上设置 replicate_do_dB:设定需要复制的数据库,多个DB用逗号分隔 replicate_Ignore_DB:设定可以忽略的数据库.—中继站忽略 replicate_Do_Table:设定需要复制的Table replicate_Ignore_Table:设定可以忽略的Table replicate_Wild_Do_Table:功能同Replicate_Do_Table,但可以带通配符来进行设置。 replicate_Wild_Ignore_Table:功能同Replicate_Do_Table,功能同replicate_Ignore_Table,可以带通配符。 在slave和master设置忽略数据库优缺点 优点是在slave端设置复制过滤机制,可以保证不会出现因为默认的数据库问题而造成Slave和Master数据不一致或复制出错的问题. 缺点是性能方面比在Master端差一些.原因在于:不管是否须要复制,事件都会被IO线程读取到Slave端,这样不仅增加了网络IO量,也给Slave端的IO线程增加了Relay Log的写入量。 主从复制几个重要的启动选项 log-slave-updates 用来配置从服务器的更新是否写入二进制日志，这个选项默认是不打开的，但是，如果这个从服务器B是服务器A的从服务器，同时还作为服务器C的主服务器，那么就需要开发这个选项，这样它的从服务器C才能获得它的二进制日志进行同步操作 master-connect-retry 是用来设置在和主服务器连接丢失的时候，重试的时间间隔，默认是60秒 read-only 是用来限制普通用户对从数据库的更新操作，以确保从数据库的安全性，不过如果是超级用户依然可以对从数据库进行更新操作 slave-skip-errors 在复制过程中，由于各种的原因，从服务器可能会遇到执行BINLOG中的SQL出错的情况，在默认情况下，服务器会停止复制进程，不再进行同步，等到用户自行来处理。slave-skip-errors的作用就是用来定义复制过程中从服务器可以自动跳过的错误号，当复制过程中遇到定义的错误号，就可以自动跳过，直接执行后面的SQL语句。 --slave-skip-errors=[err1,err2,…….|ALL] 但必须注意的是，启动这个参数，如果处理不当，很可能造成主从数据库的数据不同步，在应用中需要根据实际情况，如果对数据完整性要求不是很严格，那么这个选项确实可以减轻维护的成本 工作原理分为三步 主服务(master)把数据更改且已经提交事务的数据记录到二进制日志（binlog）中 主服务(master)发送二进制内容根据当前position 偏移量， slave通过IO线程将二进制日志 写入到 relay_log_file 中并且记录 read_master_log_pos 偏移量 slave存服务器 sql 线程读 relay_log_file 文件 并且记录偏移量relay_log_pos sql线程会重新写入到中继日志文件relay_master_log_file中 最后执行 relay_master_log_file 文件中的内容 执行SQL 记录偏移量 exe_master_log_pos 在设置主服务器时master_log_pos = xx 和master_log_file = xx master_log_file = XX 表示当前从服务器需要复制的主服务器的二进制文件名称（如果当前二进制文件未删除）会根据实际情况来选择复制那个二进制文件(slave 服务器文件 relay_log_file 中未记录xx的文件记录) stop slave ; change master to master_log_file='mysql-bin.000009' ,master_log_pos=0 ; 当前master 二进制已经发送到mysql-bin.000013 了但是可以要求master 对这台slave 从 mysql-bin.000009文件开始 （即使当前slave服务器直接设置mysql-bin.000013 master会根据当前slave relay日志来确定是否发送最早的二进制文件） master_log_file = xx 二进制日子文件的发生的偏移量地址 /var/lib/mysql/master.info 可以看到配置好的信息 change master to master_host=' ‘; 即可成功删除同步用户信息 set global sql_slave_skip_counter=N 中的N是指跳过N个event最好记的是N被设置为1时，效果跳过下一个事务。跳过第N个event后，位置若刚好落在一个事务内部，则会跳过这整个事务 一个insert/update/delete不一定只对应一个event，由引擎和日志格式决定。 SQL 线程错误一般是因为修改表结果或者主键冲突 约束 数据的一致性问题，对数据本身的修改，不涉及到一致性完整性 都是可以的 stop slave ; set global sql_slave_skip_counter=1; start slave ; show slave status; "},"server-uuid.html":{"url":"server-uuid.html","title":"server uuid","keywords":"","body":"从服务器是克隆的主服务器，server-uuid导致同步失败 //然后通过mysql生成一个uuid进行记录等会用于修改 select uuid() 5b534346-5c39-11e9-8533-0242ac110002 #替换auto.cnf 中的server-uuid server_uuid 在首次启动时 MySQL 会调用 generate_server_uuid() 自动生成一个 server_uuid，并且保存到 auto.cnf 文件 —— 这个文件目前存在的唯一目的就是保存 server_uuid。 在 MySQL 再次启动时会读取 auto.cnf 文件，继续使用上次生成的 server_uuid。 使用 SHOW 命令可以查看 MySQL 实例当前使用的 server_uuid​： SHOW GLOBAL VARIABLES LIKE 'server_uuid'; auto.cnf [auto] server-uuid=5b534346-5c39-11e9-8533-0242ac110002 全局唯一的 server_uuid 的一个好处是：可以解决由 server_id 配置冲突带来的 MySQL 主备复制的异常终止 在 MySQL 5.6，Slave 向 Master 申请 binlog 时，会首先发送自己的 server_uuid，Master 用 Slave 发送的 server_uuid 代替 server_id （MySQL 5.6 之前的方式）作为 kill_zombie_dump_threads 的参数，终止冲突或者僵死的 BINLOG_DUMP 线程。 "},"suo.html":{"url":"suo.html","title":"锁","keywords":"","body":" 锁粒度 一种提高共享资源并发性的方式就是让锁定对象更有选择性。尽量只锁定需要修改的部分数据，而不是所有的资源。更理想的方式是，只对会修改的数据片进行精确的锁定。任何时候，在给定的资源上，锁定的数据量越少，则系统的并发程度越高，只要相互之间不发生冲突即可。 问题是加锁也需要消耗资源。锁的各种操作，包括获得锁、检查锁是否已经解除、释放锁等，都会增加系统的开销。如果系统花费大量的时间来管理锁，而不是存取数据，那么系统的性能可能会因此受到影响。 所谓的锁策略，就是在锁的开销和数据的安全性之间寻求平衡，这种平衡当然也会影响到性能。大多数商业数据库系统没有提供更多的选择，一般都是在表上施加行级锁（rowlevellock），并以各种复杂的方式来实现，以便在锁比较多的情况下尽可能地提供更好的性能。 而MySQL则提供了多种选择。每种MySQL存储引擎都可以实现自己的锁策略和锁粒度。 在存储引擎的设计中，锁管理是个非常重要的决定。将锁粒度固定在某个级别，可以为某些特定的应用场景提供更好的性能，但同时却会失去对另外一些应用场景的良好支持。好在MySQL支持多个存储引擎的架构，所以不需要单一的通用解决方案。下面将介绍两种最重要的锁策略。 读写锁 在处理并发读或者写时，可以通过实现一个由两种类型的锁组成的锁系统来解决问题。这两种类型的锁通常被称为共享锁（sharedlock）和排他锁（exclusivelock），也叫读锁（readlock）和写锁（writelock）。 "},"suo/biao-suo.html":{"url":"suo/biao-suo.html","title":"表锁","keywords":"","body":"表锁是MySQL中最基本的锁策略，并且是开销最小的策略。表锁非常类似于前文描述的邮箱加锁机制：它会锁定整张表。一个用户在对表进行写操作（插入、删除、更新等）前，需要先获得写锁，这会阻塞其他用户对该表的所有读写操作。只有没有写锁时，其他读取的用户才能获得读锁，读锁之间是不相互阻塞的。 在特定的场景中，表锁也可能有良好的性能。例如，READLOCAL表锁支持某些类型的并发写操作。另外，写锁也比读锁有更高的优先级，因此一个写锁请求可能会被插入到读锁队列的前面（写锁可以插入到锁队列中读锁的前面，反之读锁则不能插入到写锁的前面）。尽管存储引擎可以管理自己的锁，MySQL本身还是会使用各种有效的表锁来实现不同的目的。例如，服务器会为诸如ALTERTABLE之类的语句使用表锁，而忽略存储引擎的锁机制。 "},"suo/biao-suo/readlocal.html":{"url":"suo/biao-suo/readlocal.html","title":"READLOCAL","keywords":"","body":""},"suo/xing-ji-suo.html":{"url":"suo/xing-ji-suo.html","title":"行级锁","keywords":"","body":"行级锁可以最大程度地支持并发处理（同时也带来了最大的锁开销）。众所周知，在InnoDB和XtraDB，以及其他一些存储引擎中实现了行级锁。行级锁只在存储引擎层实现，而MySQL服务器层没有实现。服务器层完全不了解存储引擎中的锁实现。所有的存储引擎都以自己的方式显现了锁机制 "},"suo/xing-ji-suo/ssuo.html":{"url":"suo/xing-ji-suo/ssuo.html","title":"S(共享锁)","keywords":"","body":"共享锁 "},"suo/xing-ji-suo/xsuo.html":{"url":"suo/xing-ji-suo/xsuo.html","title":"X(排他锁)","keywords":"","body":""},"suo/xing-ji-suo/xing-suo-de-suan-fa.html":{"url":"suo/xing-ji-suo/xing-suo-de-suan-fa.html","title":"算法","keywords":"","body":"InnoDB存储引擎有3种行锁的算法，其分别是： RecordLock：单个行记录上的锁。 GapLock：间隙锁，锁定一个范围，但不包含记录本身。 NextKeyLock∶GapLock+RecordLock，锁定一个范围，并且锁定记录本身。 "},"suo/xing-ji-suo/recordlock.html":{"url":"suo/xing-ji-suo/recordlock.html","title":"RecordLock","keywords":"","body":"RecordLock总是会去锁住索引记录，如果InnoDB存储引擎表在建立的时候没有设置任何一个索引，那么这时InnoDB存储引擎会使用隐式的主键来进行锁定。 "},"suo/xing-ji-suo/gaplock.html":{"url":"suo/xing-ji-suo/gaplock.html","title":"GapLock","keywords":"","body":""},"suo/xing-ji-suo/nextkeylock.html":{"url":"suo/xing-ji-suo/nextkeylock.html","title":"NextKeyLock","keywords":"","body":"NextKeyLock是结合了GapLock和RecordLock的一种锁定算法，在NextKeyLock算法下，InnoDB对于行的查询都是采用这种锁定算法，但是当查询的索引含有唯一属性时，InnoDB存储引擎会对NextKeyLock进行优化，将其降级为RecordLock，即仅锁住索引本身，而不是范围。 NextKeyLock 锁定一个范围，并且锁定记录本身 CREATE TABLE `a` ( `id` int(11) NOT NULL, `vale` varchar(45) DEFAULT NULL, PRIMARY KEY (`id`), KEY `vale_index` (`vale`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8 NextKeyLock锁降级为RecordLock(和索引有关系) insert into a select 1 ; insert into a select 2 ; insert into a select 4 ; #事务A begin #id是唯一索引，所以NextKeyLock锁降级为RecordLock只锁住 id=4的行 select * from a where id = 4 for update ; #事务B 可以进行插入不会阻塞 begin insert into a select 3 ; commit; #事务A 在提交事务 commit; "},"suo/yi-zhi-xing-fei-suo-ding-du.html":{"url":"suo/yi-zhi-xing-fei-suo-ding-du.html","title":"一致性非锁定读","keywords":"","body":"一致性的非锁定读（consistentnonlockingread）是指InnoDB存储引擎通过行多版本控制（multiversioning）的方式来读取当前执行时间数据库中行的数据。如果读取的行正在执行DELETE或UPDATE操作，这时读取操作不会因此去等待行上锁的释放。相反地，InnoDB存储引擎会去读取行的一个快照数据 "},"suo/yi-zhi-xing-suo-ding-du.html":{"url":"suo/yi-zhi-xing-suo-ding-du.html","title":"一致性锁定读","keywords":"","body":"可以显式地对数据库读取操作进行加锁以保证数据逻辑的一致性 在默认配置下，即事务的隔离级别为REPEATABLEREAD模式下，InnoDB存储引擎的SELECT操作使用一致性非锁定读。 InnoDB存储引擎对于SELECT语句支持两种一致性的锁定读操作： #对读取的行记录加一个X锁，其他事务不能对已锁定的行加上任何锁。 SELECT…FOR UPDATE #对读取的行记录加一个S锁，其他事务可以向被锁定的行加S锁，但是如果加X锁，则会被阻塞 SELECT…LOCK IN SHARE MODE "},"suo/shi-wu.html":{"url":"suo/shi-wu.html","title":"事务","keywords":"","body":"ACID，指数据库事务正确执行的四个基本要素的缩写。包含：原子性（Atomicity）、一致性（Consistency）、隔离性（Isolation）、持久性（Durability） 原子性(A) 指整个数据库事务是不可分割的工作单位。只有使事务中所有的数据库操作都执行成功，才算整个事务成功。事务中任何一个SQL语句执行失败，已经执行成功的SQL语句也必须撤销，数据库状态应该退回到执行事务前的状态。 一致性(C)指事务将数据库从一种状态转变为下一种一致的状态。在事务开始之前和事务结束以后，数据库的完整性约束没有被破坏。例如，在表中有一个字段为姓名，为唯一约束，即在表中姓名不能重复。如果一个事务对姓名字段进行了修改，但是在事务提交或事务操作发生回滚后，表中的姓名变得非唯一了，这就破坏了事务的一致性要求，即事务将数据库从一种状态变为了一种不一致的状态。因此，事务是一致性的单位，如果事务中某个动作失败了，系统可以自动撤销事务——返回初始化的状态。也就是说 第一个事务 对 name=1 进行了 inster，第二事务也进行了inster 或者对其他数据的更新 name=1 那么在提交事务后 name就不是唯一了 ，所以一致性保证了 唯一 谁最后提交就失败 ，通过约束可以使数据库 一致性和完整性 隔离性(I)事务的隔离性要求每个读写事务的对象对其他事务的操作对象能相互分离，即该事务提交前对其他事务都不可见，通常这使用锁来实现。 持久性(D)事务一旦提交，其结果就是永久性的。即使发生宕机等故障，数据库也能将数据恢复。 autocommitMySQL默认操作模式就是autocommit自动提交模式。这就表示除非显式地开始一个事务，否则每个查询都被当做一个单独的事务自动执行。我们可以通过设置autocommit的值改变是否是自动提交autocommit模式。通过以下命令可以查看当前autocommit模式 mysql> show variables like 'autocommit';+---------------+-------+| Variable_name | Value |+---------------+-------+| autocommit | ON |+---------------+-------+1 row in set (0.04 sec) 从查询结果中，我们发现Value的值是ON，表示autocommit开启。我们可以通过以下SQL语句改变这个模式临时mysql> set autocommit = 0; 永久 在mysql.cnf 文件中 添加 autocommit=0 ,值0和OFF都是一样的，当然，1也就表示ON。通过以上设置autocommit=0，则用户将一直处于某个事务中，直到执行一条commit提交或rollback语句才会结束当前事务重新开始一个新的事务。 COMMIT WORK 也可以写为COMMIT， 不过这二者几乎是等价的，会提交事务， 并使得已对数据库做的所有修改成为永久性的。ROLLBACK 也可以写为ROLLBACK WORK，但是二者几乎是等价的。回滚会结束用户的事务，并撤销正在进行的所有未提交的修改。step 1 : savepoint savepoint_name;、 做标记step 2 :rollbak to savepoint savepoint_name;回滚到标记点setp 3 :release savepoint savepoint_name;解除标记 隔离级别 为了兼顾并发效率和异常控制，在标准SQL规范中，定义了4个事务隔离级别，（Oracle和SQLSERER对标准隔离级别有不同的实现 ） Read Uncommitted 直译就是\"读未提交\",意思就是即使一个更新语句没有提交,但是别 的事务可以读到这个改变.这是很不安全的. Read Committed 直译就是\"读提交\",意思就是语句提交以后即执行了COMMIT以后 别的事务就能读到这个改变. Repeatable Read 直译就是\"可以重复读\",这是说在同一个事务里面先后执行同一个 查询语句的时候,得到的结果是一样的. Serializable 直译就是\"序列化\",意思是说这个事务执行的时候不允许别的事务并发执行 -- 查询事物隔离级别 SELECT @@tx_isolation; --设置当前session的事物隔离级别为read uncommitted set session transaction isolation level read uncommitted; redo和undo的作用都可以视为是一种恢复操作 redo恢复提交事务修改的页操作，而undo回滚行记录到某个特定版本。 因此两者记录的内容不同，redo通常是物理日志，记录的是页的物理修改操作。 undo是逻辑日志，根据每行记进行记录 原子性,一致性,持久性通过数据库的redo_log和undo_log来完成。 redo_log 称为重做日志，用来保证事务的原子性和持久性。undo_log 用来保证事务的一致性 脏读一个事务可以读到另外一个事务中未提交的数据， 则显然违反了数据库的隔离性。 不可重复读 — 表示读以提交的事务不可重复读是第一事务还未提交，而第二事务对数据添加或者更新并且提交事务，导致第一个事务再次查询出现不一样的结果，但是mysql 通过Next-Key Lock 避免了。 可重复读 —针对的是快照读 在第一个事务A中对同一条记录查询结果都是一样的，即使在其他事务B中对这条数据进行update 操作并且提交，而在第一次事务A中再次查询看到的结果都是和前两次一样(前提事务A还未提及事务) begin ; select * from a1 where id = 1 ; — 结果name=1 ——在其事务B中对这条数据进行了update name=2 且已经提了 select * from a1 where id = 1 ; --结果还是name=1 -— update 更新的是原始数据，不是快照 ，而select在隔离级别为可重复时读的是当前事务的快照 update a1 set name=name+’2’ where id = 1 ; select * from a1 where id = 1 ; --结果还是name=’22' commit ； select * from a1 where id = 1 ; —结果变成了name=2 在select 走的快照读，而update Tex1 set arg = arg+100 where id = 2 ; 其中arg+100是当前读 update ,inster ,delete ,for update 或者显示加锁操作都是当前读，那么什么是快照读，什么是当前读？ 快照读 数据库通过快照数据的方式，来隔离事务之间数据可见性问题，在第一次通过条件查询数据的时候数据库引擎会为这行记录快照一份当前的记录（原始记录），如果接下来其他事务对这行数据进行update 的时候修改操作时候数据库引擎同样会在创建一份快照并且对符合条件的原始记录加锁 事务ID指向当前事务ID，其他事务无法在进行加锁操作了 ，所以快照表示读当前事务下的快照记录 select 都是快照读，切都是读当前事务下的快照记录 而for update ,lock都是读原始记录并且加锁 当前读 表示对原始记录 进行加锁操作的时候都是当前读，返回的都是原始记录，例如 事务隔离等级为可重复读 事务A 对这行数据查询得到的目列结果 1 ，而事务B对这条数据update 成2 或者delete 且事务提交，那么事务A select 快照读，读到的还是结果为1 select * for update 读到的就2或者没有数据已经删除-也就是当前读 也会包含inster新插入的记录(会锁住自己的主键)即使当前事务未提交当前读都会被阻塞（符合当前加锁的记录的条件） inster(name = 1) 事务A 插入一条（主键加锁）未提交事务 update set name = 2 where name = 1 事务B 进行更新（会被阻塞） 读未提交的级别 逻辑意义的丢失更新问题（数据库是不会允许 同时修改一条数据的） 事务 T1 查询 一行 数据， 放入 本地 内存， 并 显示 给 一个 终端 用户 User1。 事务 T2 也 查询 该 行 数据， 并将 取得 的 数据 显示 给 终端 用户 User2。 User1 修改 这 行 记录， 更新 数据库 并提 交。 User2 修改 这 行 记录， 更新 数据库 并提 交。 for update（ 不能使用lock in shard mode因为目的就是排除其他事务） 锁住要更新的对象 在进行其他计算或者更新以及 预期值和当前值比较 失败了 查询查询更新 MySQL有三种锁的级别 页级 表级 行级 三种算法 Gap Lock ：间隙锁，锁定一个范围但不包含纪录本身。 Record Lock： 单个行纪录上的锁。 Next-Key Lock：是由Gap 和Record 锁 合并而成。 InnoDB对于行的查询都是采用这种锁定算法, 但是如果索引中包含唯一索引，InnoDB对Next-KeyLock锁升级，降级为Reord Lock 锁。 Next-Key Lock 是为了解决幻像而设计的。 1、共享锁（S Lock），允许事务读一行数据2、排他锁（X Lock)，允许事务更新或者删除一行数据共享锁和排他锁的兼容如下图所示 锁的触发 insert into 对外键表锁的触发 ---共享锁（1）外键Id是主键索引 begin ; -- idd 是外健 a1 的id insert into `Tx`.`a2` ( `adder`, `idd`, `tel`) values ( '123', '1', '123'); 重新开启一个事务 begin ; — S 锁可以执行 select * from a1 where id = 1 LOCK IN SHARE MODE ; — X 就需要等待a2 提交事务才可以 select * from a1 where id = 1 for update ; — 如果是同一个事务中 是可以执行的，也就是说锁的可以重入的 因为在外键的插入 对于数据的隔离性要求较高，在插入前需要扫描父表中的记录是否存在，所以，在外键的插入上，InnoDB会使用加S锁的方式来实现 对于删除或者更新 不会对外健影响。 解决办法: 添加一张中间表 用于关联主表，这样可以减少对主表S锁 （2）显式对读进行加锁 如使用 select --- for update 或者 select --- lock in share mode （3）insert into 检查主键冲突属于表S锁 插入操作会依据这个自增长的计数器值加1赋予自 增长 列。 这个 实现 方式 称做 AUTO- INC Locking。 这种 锁 其实 是 采用 一种 特殊 的 表 锁 机制， 为了 提高 插入 的 性能， 锁 不是 在 一个 事务 完成 后才 释放， 而是 在 完 成对 自 增长 值 插入 的 SQL 语句 后 立即 释放。 InnoDB锁的范围取决于索引类型 Record Lock：单个行记录上的锁。 （update 唯一索引，主键索引） Gap Lock：间隙锁，锁定一个范围，但不包含记录本身（锁住的是数据之间位置,对唯一索引插入 insert into ） 事务A insert into A(mac)values(‘1') 唯一属性 未提交事务事务B insert into A(mac)values(‘1') 执行时sql异常如：mac_unique唯一索引名称 违法约束 事务A在执行insert into 过程中，会锁组它左右两边（索引是有序排列的）索引项，不包括自己（id主键生成就放开锁），事务B在执行同样的SQL需要建立在唯一索引上添加数据所以会被阻塞 Next-key Lock:Gap Lock+Record Lock，锁定一个范围，并且锁定记录本身 （一般索引） InnoDB对于行的查询或者普通索引都是采用这种锁定算法, 但是如果索引中包含唯一索引，InnoDB对Next-KeyLock锁降级为Reord Lock锁。 Next-Key Lock 是为了解决幻像而设计的，索引列不是唯一索引且是当前读或者数据更新都会加间隙锁 select * for update 如事务级别是可重复读(id不是主键 name是) 并且不存在间隙锁 会怎么样： begin ; select * from T1 where id = 10 for update ; 返回的结果是2条 假如其他事务对T1 表做 inster id=10 那么在第二次 select for update 过程中就会变成了3条数据 这就出现了幻读了 update 更新时候加间隙锁防止inster 插入 就会出现幻读 加间隙锁 begin update T1 set id = 10 where id = 10 ; 会更新两条数据 假如其他事务inster (id = 10) ;提交事务 那么就会出现问题 select * from T1 where id = 10 for update 变成3条了 inster 插入的时候加间隙锁防止接下来其他事务update的更新，如果不加就出现幻读情况 begin Inster into (id = 10) 事务A中 事务B 执行update where id = 10 ;那么 索引分四类索引的类型决定了当前读时锁的范围，包括insert into 时外键锁定共享锁（S） 无索引 当前读-表锁 REPEATABLE-READ(可重复读) update Client set name = 'u1' where deviceName = 'X220-1' 若deviceName 列上没有索引SQL会走聚簇索引(id)全扫描进行过滤，全部加锁采用的是表锁，阻止inster, delete update ,for update,共享锁 update Client set name = 'u1' where deviceName = 'X220-1' and mac = '80:e6:50:27:6f:54’ ; mac 是唯一索引SQL走的是 唯一索引 也就锁住一条 index(普通的索引,数据可以重复) update inster Delete for update / lock in shan mode fulltext(全文索引，用来对大表的文本域(char，varchar，text)进行索引语法和普通索引一样) unique(唯一索引,唯一索引,要求所有记录都唯一) primary key (主键索引,也就是在唯一索引的基础上相应的列必须为主键) 外健索引 事务隔离机制的不同体现在事务之间数据可见的不同 而可见都是针对快照读，而锁（update，inster，for update）是保证了对数据读的执行顺序（先根据条件找到索引在对次加锁） ，而约束保证了 数据的完整性和一致性，回滚操作体现了原子性上（要不全部提交提交失败回归所有的更新） 死锁解决办法 死锁产生是一个锁的加锁等待另外一个锁的释放就会产生 等待其他事务释放锁 在事务A中更新或者显示的加锁了，但是事务未提交 在事务B中对这条数据进行更新或者加锁，那么就会出现了等待 直到事务A 提交或者回滚了 才执行 循环嵌套了对方的锁 例如同时执行完第1步接下来就发生死锁 事务A select * from A where id = 1 for update —1 select * from B where id = 1 for update —2 事务B select * from B where id = 1 for update — 1 select * from A where id = 1 for update — 2 解决死锁的最简单的办法是超时等待在innodb中可以通过 innodb lock wait_ timeout = 1设置发生死锁等待时间 回滚事务或者操作 单位是秒杀。 innodb_rollback_on_timeout = 1| 0 1表示回滚整个事务 0表示回滚当前语句不回滚事务 继续执行下面的SQL语句 information_schema 数据库中 SHOW ENGINE INNODB STATUS 引擎表相信 INNODB_TRX 事务表，纪录待提交的事务信息。 INNODB_ LOCK_WAITS 锁等待消息 INNODB_ LOCK 锁消息 一致性的非锁定读（快照读）是指InnoDB存储引擎通过行多版本控制（ multi versioning） 的方式来读取当前执行时间数据库中行的数据。 如果读取的行 正在执行DELETE 或 UPDATE 操作， 这时读取操作不会因此去等待行上锁的释放。而是读一个快照。 一致性锁定读（当前读） InnoDB存储引擎的SELECT操作使用一致性非锁定读。 但是在某些情况下， 用户需要显式地对数据库读取操作进行加锁以保证 数据逻辑的一致性。 而这要求数据库支持加锁语句， 即使是对于SELECT的只读操作。 对于SELECT语句支持两种一致性的锁定读操作： ❑ SELECT… FOR UPDATE 排它锁 ❑ SELECT… LOCK IN SHARE MODE 共享锁 "},"suo/shi-wu/ge-li-ji-bie.html":{"url":"suo/shi-wu/ge-li-ji-bie.html","title":"隔离级别","keywords":"","body":""},"suo/shi-wu/ge-li-ji-bie/repeatableread.html":{"url":"suo/shi-wu/ge-li-ji-bie/repeatableread.html","title":"REPEATABLE READ（可以重复读）","keywords":"","body":"REPEATABLEREAD解决了脏读的问题。该级别保证了在同一个事务中多次读取同样记录的结果是一致的。但是理论上，可重复读隔离级别还是无法解决另外一个幻读（PhantomRead）的问题。所谓幻读，指的是当某个事务在读取某个范围内的记录时，另外一个事务又在该范围内插入了新的记录，当之前的事务再次读取该范围的记录时，会产生幻行（PhantomRow）。InnoDB和XtraDB存储引擎通过多版本并发控制（MVCC，MultiversionConcurrencyControl）解决了幻读的问题。本章稍后会做进一步的讨论。 "},"suo/shi-wu/ge-li-ji-bie/read-committed.html":{"url":"suo/shi-wu/ge-li-ji-bie/read-committed.html","title":"READ COMMITTED","keywords":"","body":"大多数数据库系统的默认隔离级别都是READCOMMITTED（但MySQL不是）。READCOMMITTED满足前面提到的隔离性的简单定义：一个事务开始时，只能“看见”已经提交的事务所做的修改。换句话说，一个事务从开始直到提交之前，所做的任何修改对其他事务都是不可见的。这个级别有时候也叫做不可重复读（nonrepeatableread），因为两次执行同样的查询，可能会得到不一样的结果。 "},"suo/shi-wu/ge-li-ji-bie/read-uncommitted.html":{"url":"suo/shi-wu/ge-li-ji-bie/read-uncommitted.html","title":"READ UNCOMMITTED","keywords":"","body":"在READUNCOMMITTED级别，事务中的修改，即使没有提交，对其他事务也都是可见的。事务可以读取未提交的数据，这也被称为脏读（DirtyRead）。这个级别会导致很多问题，从性能上来说，READUNCOMMITTED不会比其他的级别好太多，但却缺乏其他级别的很多好处，除非真的有非常必要的理由，在实际应用中一般很少使用 "},"suo/shi-wu/ge-li-ji-bie/serializable.html":{"url":"suo/shi-wu/ge-li-ji-bie/serializable.html","title":"SERIALIZABLE","keywords":"","body":"SERIALIZABLE是最高的隔离级别。它通过强制事务串行执行，避免了前面说的幻读的问题。简单来说，SERIALIZABLE会在读取的每一行数据上都加锁，所以可能导致大量的超时和锁争用的问题。实际应用中也很少用到这个隔离级别，只有在非常需要确保数据的一致性而且可以接受没有并发的情况下，才考虑采用该级别。 "},"shi-wu.html":{"url":"shi-wu.html","title":"事务与锁","keywords":"","body":""},"wai-jian-he-suo.html":{"url":"wai-jian-he-suo.html","title":"外键和锁","keywords":"","body":"对于外键值的插入或更新，首先需要查询父表中的记录，即SELECT父表。但是对于父表的SELECT操作，不是使用一致性非锁定读的方式，因为这样会发生数据不一致的问题，因此这时使用的是一致性锁定读S锁方式，即主动对父表加一个S锁。如果这时父表上已经这样加X锁，子表上的操作会被阻塞， "},"suo-yin.html":{"url":"suo-yin.html","title":"索引与约束","keywords":"","body":"KEY key 是数据库的物理结构，它包含两层意义，一是约束（偏重于约束和规范数据库的结构完整性），二是索引（辅助查询用的）。包括primary key, unique key, foreign key 等。 primary key（主键索引约束）有两个作用，一是约束作用（constraint），用来规范一个存储主键和唯一性，但同时也在此key上建立了一个index； unique key（唯一索引约束）也有两个作用，一是约束作用（constraint），规范数据的唯一性，但同时也在这个key上建立了一个index； foreign key（外键索引约束）也有两个作用，一是约束作用（constraint），规范数据的引用完整性，但同时也在这个key上建立了一个index； mysql中的key是同时具有constraint(约束)和index的意义。 INDEX index是数据库的物理结构，它只是辅助查询的，它创建时会在另外的表空间（mysql中的innodb表空间）以一个类似目录的结构存储。索引要分类的话，分为前缀索引、全文本索引等；因此，索引只是索引，它不会去约束索引的字段的行为。 例如：create table t(id int, index inx_tx_id (id)); 总结 我们说索引分类，分为主键索引、唯一索引、普通索引(这才是纯粹的index)等，也是基于是不是把index看作了key。比如 create table t(id int, unique index inx_tx_id (id)); 这里的index相当于key的效果 "},"suo-yin/a.html":{"url":"suo-yin/a.html","title":"索引","keywords":"","body":" 最常见的BTree索引，按照顺序存储数据，所以MySQL可以用来做ORDERBY和GROUPBY操作。因为数据是有序的，所以BTree也就会将相关的列值都存储在一起。最后，因为索引中存储了实际的列值，所以某些查询只使用索引就能够完成全部查询。据此特性，总结下来索引有如下三个优点： 索引大大减少了服务器需要扫描的数据量。 索引可以帮助服务器避免排序和临时表。 索引可以将随机I/O变为顺序I/O。 如何评价一个索引是否适合某个查询的“三星系统”（threestarsystem）： 索引将相关的记录放到一起则获得一星； 如果索引中的数据顺序和查找中的排列顺序一致则获得二星； 如果索引中的列包含了查询中需要的全部列则获得“三星”。 索引并不总是最好的工具 只有当索引帮助存储引擎快速查找到记录带来的好处大于其带来的额外工作时，索引才是有效的。对于非常小的表，大部分情况下简单的全表扫描更高效。对于中到大型的表，索引就非常有效。但对于特大型的表，建立和使用索引的代价将随之增长。可以使用分区技术 如果表的数量特别多，可以建立一个元数据信息表，用来查询需要用到的某些特性。例如执行那些需要聚合多个应用分布在多个表的数据的查询，则需要记录“哪个用户的信息存储在哪个表中”的元数据，这样在查询时就可以直接忽略那些不包含指定用户信息的表。对于大型系统，这是一个常用的技巧。事实上，Infobright就是使用类似的实现。对于TB级别的数据，定位单条记录的意义不大，所以经常会使用块级别元数据技术来替代索引。 B+树索引的本质就是B+树在数据库中的实现 但是B+索引在数据库中有一个特点是高扇出性每页四条记录，因此在数据库中B+树的高度一般都在2 ～ 4层， 这也就是说查找某一键值的行记录时最多只需要2 到4次IO， 这倒不错。 因为当前一般的机械磁盘每秒至少可以做100次IO， 2 ～ 4次的IO意味着查询时间只需0. 02 ～ 0. 04秒。 数据库中的B+树索引可以分为聚集索引（ clustered inex）和辅助索引（ secondary index）但是不管是聚集还是辅助的索引，其内部都是B+树的，即高度平衡的， 叶子节点存放着所有的数据。 聚集索引与辅助索引不同的是， 叶子节点存放的是否是一整行的信息。 如上图是table a （id, name） 的记录总共15条，ID=5～95 由于ID是聚集索引，所以叶子节点存放是id所对应的一行数据，且这些数据是顺序存放的 ,那么在数据中B+树索引表示如图 每个节点树的索引值就是id的值，而最底层的叶子节点就是存放一行的数据如下图： select * from a where id = 5 id=5的的行数据在最左边其次是 id =10的 以此类推，所以如果查询id=60的 那么，首先对index Page 取中判断55 索引分为聚簇索引和非聚簇索引两种 聚簇索引是按照数据存放的物理位置为顺序的，而非聚簇索引就不一样了,聚簇索引能提高多行检索的速度，而非聚簇索引对于单行的检索很快。 -- 关闭索引 ALTER TABLE `test` DISABLE KEYS ; -- 开启索引 ALTER TABLE `test` ENABLE KEYS; "},"suo-yin/bu-shi-yong-suo-yin-de-qing-kuang/suo-yin-xuan-ze-xing.html":{"url":"suo-yin/bu-shi-yong-suo-yin-de-qing-kuang/suo-yin-xuan-ze-xing.html","title":"索引选择性","keywords":"","body":"索引的选择性是指，不重复的索引值（也称为基数，cardinality）和数据表的记录总数（#T）的比值，范围从1/#T到1之间。索引的选择性越高则查询效率越高，因为选择性高的索引可以让MySQL在查找时过滤掉更多的行。唯一索引的选择性是1，这是最好的索引选择性，性能也是最好的。 "},"suo-yin/suo-yin-xuan-ze.html":{"url":"suo-yin/suo-yin-xuan-ze.html","title":"Cardinality(基数)","keywords":"","body":"高选择性 并不是在所有的查询条件中出现的列都需要添加索引。对于什么时候添加B+树索引，如果某个字段的取值范围很广，几乎没有重复，即属于高选择性，则此时使用B+树索引是最适合的--建立索引的前提是列中的数据是高选择性的，这对数据库来说才具有实际意义 如：性别 SELECT* FROM student WHERE sex='M' 按性别进行查询时，可取值的范围一般只有'M'、'F'。因此上述SQL语句得到的结果可能是该表50%的数据（假设男女比例1∶1），这时添加B+树索引是完全没有必要的。 查看索引是否是高选择性的呢？ 通过SHOW INDEX结果中的列Cardinality来观察。Cardinality值非常关键，表示索引中不重复记录数量的预估值,该值是一个预估值不是精确值 select count(*) from t_a ; # 15 show index from t_a ; #'t_a','0','PRIMARY','1','id','A','15',NULL,NULL,'','BTREE','','' # cardinality =15 select cardinality/row_n from dual # 15/15 = 1 cardinality/row 尽可能地接近1。如果非常小，那么考虑是否还有必要创建索引 Cardinality统计的实现 因为MySQL数据库中有各种不同的存储引擎，而每种存储引擎对于B+树索引的实现又各不相同，所以对Cardinality的统计是放在存储引擎层进行的。 Cardinality信息的统计和更新操作是通过采样的方法计算，默认InnoDB存储引擎对8个叶子节点进行采用。采样的过程如下： 取得B+树索引中叶子节点的数量，记为A。 随机取得B+树索引中的8个叶子节点。统计每个页不同记录的个数，即为P1，P2，…，P8。 根据采样信息给出Cardinality的预估值：Cardinality=（P1+P2+…+P8）*A/8。 在InnoDB存储引擎中，Cardinality值是通过对8个叶子节点预估而得的，不是一个实际精确的值 且是随机，即使表更改任何数据每次统计值可能都不一样。 InnoDB存储引擎内部对更新Cardinality信息的策略为： 表中1/16的数据已发生过变化。 自从上次统计Cardinality信息后，表中1/16的数据已经发生过变化，这时需要更新Cardinality信息 stat_modified_counter＞2000000000。 如果对表中某一行数据频繁地进行更新操作，这时表中的数据实际并没有增加，实际发生变化的还是这一行数据，则第一种更新策略就无法适用这这种情况。故在InnoDB存储引擎内部有一个计数器stat_modified_counter，用来表示发生变化的次数，当stat_modified_counter大于2000000000时，则同样需要更新Cardinality信息 "},"suo-yin/ju-ji-suo-yin.html":{"url":"suo-yin/ju-ji-suo-yin.html","title":"聚集索引","keywords":"","body":"聚簇索引并不是一种单独的索引类型，而是一种数据存储方式。具体的细节依赖于其实现方式，但InnoDB的聚簇索引实际上在同一个结构中保存了BTree索引和数据行。当表有聚簇索引时，它的数据行实际上存放在索引的叶子页（leafpage）中。 术语“聚簇”表示数据行和相邻的键值紧凑地存储在一起 。因为无法同时把数据行存放在两个不同的地方，所以一个表只能有一个聚簇索引 "},"suo-yin/fu-zhu-suo-yin.html":{"url":"suo-yin/fu-zhu-suo-yin.html","title":"辅助索引(二级索引)","keywords":"","body":"对于InnoDB存储引擎的辅助索引而言，由于其包含了主键信息，因此其叶子节点存放的数据为（primarykey1，primarykey2，…，key1，key2，…）。例如，下列语句都可仅使用一次辅助联合索引来完成查询： SELECT key2 FROM table WHERE key1=xxx； SELECT primary key2,key2 FROM table WHERE key1=xxx； SELECT primary key1,key2 FROM table WHERE key1=xxx； SELECT primary key1,primary key2，key2 FROM table WHERE key1=xxx； "},"suo-yin/ha-xi-suan-fa.html":{"url":"suo-yin/ha-xi-suan-fa.html","title":"哈希索引","keywords":"","body":"哈希索引（hashindex）基于哈希表实现，只有精确匹配索引所有列的查询才有效 。对于每一行数据，存储引擎都会对所有的索引列计算一个哈希码（hashcode），哈希码是一个较小的值，并且不同键值的行计算出来的哈希码也不一样。哈希索引将所有的哈希码存储在索引中，同时在哈希表中保存指向每个数据行的指针。 在MySQL中，只有Memory引擎显式支持哈希索引。这也是Memory引擎表的默认索引类型，Memory引擎同时也支持BTree索引。 Memory引擎是支持非唯一哈希索引的 CREATE TABLE `testhash` ( `fname` varchar(50) NOT NULL, `lname` varchar(50) NOT NULL, KEY `fname` (`fname`) USING HASH ) ENGINE=MEMORY DEFAULT CHARSET=utf8 ; insert into testhash values(\"ccc\",\"ddd\"); ... select * from testhash where fname = 'ccc' ; MySQL先计算'ccc'的哈希值，并使用该值寻找对应的记录指针。因为HASH（'ccc'）=8784，所以MySQL在索引中查找8784，可以找到指向第3行的指针，最后一步是比较第三行的值是否为'ccc'，以确保就是要查找的行。 因为索引自身只需存储对应的哈希值，所以索引的结构十分紧凑，这也让哈希索引查找的速度非常快,但是哈希索引也有限制： 哈希索引只包含哈希值和行指针，而不存储字段值，所以不能使用索引中的值来避免读取行。不过，访问内存中的行的速度很快，所以大部分情况下这一点对性能的影响并不明显。哈希索引数据并不是按照索引值顺序存储的，所以也就无法用于排序。哈希索引也不支持部分索引列匹配查找，因为哈希索引始终是使用索引列的全部内容来计算哈希值的。例如，在数据列（A,B）上建立哈希索引，如果查询只有数据列A，则无法使用该索引。哈希索引只支持等值比较查询，包括=、IN()、（注意<>和是不同的操作）。也不支持任何范围查询，例如WHEREprice>100。访问哈希索引的数据非常快，除非有很多哈希冲突（不同的索引列值却有相同的哈希值）。当出现哈希冲突的时候，存储引擎必须遍历链表中所有的行指针，逐行进行比较，直到找到所有符合条件的行。如果哈希冲突很多的话，一些索引维护操作的代价也会很高。例如，如果在某个选择性很低（哈希冲突很多）的列上建立哈希索引，那么当从表中删除一行时，存储引擎需要遍历对应哈希值的链表中的每一行，找到并删除对应行的引用，冲突越多，代价越大。因为这些限制，哈希索引只适用于某些特定的场合。而一旦适合哈希索引，则它带来的性能提升将非常显著。举个例子，在数据仓库应用中有一种经典的“星型”schema，需要关联很多查找表，哈希索引就非常适合查找表的需求。 "},"suo-yin/fu-gai-suo-yin.html":{"url":"suo-yin/fu-gai-suo-yin.html","title":"覆盖索引","keywords":"","body":"InnoDB存储引擎支持覆盖索引（索引覆盖该查询的记录），即从辅助索引中就可以得到查询的记录，而不需要查询聚集索引中的记录。使用覆盖索引的两个好处，如下 第一好处是辅助索引 覆盖了主键信息（叶子节点都包含了主键的值） show index from ad_account ; #'ad_account','0','PRIMARY','1','id','A','84240',NULL,NULL,'','BTREE','','' #'ad_account','1','user_id_index','1','user_id','A','84240',NULL,NULL,'YES','BTREE','','' #由于辅助索引其包含了主键信息，c存放的数据为（primarykey1，primarykey2，…，key1，key2，…）所以user_id_index索引覆盖 explain SELECT id FROM ad_account where user_id = 100; #'1','SIMPLE','ad_account',NULL,'ref','user_id_index','user_id_index','5','const','1','100.00','Using index' 另一个好处是对某些统计，辅助索引远小于聚集索引，选择辅助索引可以减少IO操作。 #辅助索引(user_id_index) 小于聚集索引减少大量的IO操作，故其mysql优化器使用user_id_index 进行统计 explain select count(*) from ad_account ; # '1','SIMPLE','ad_account',NULL,'index',NULL,'user_id_index','5',NULL,'84240','100.00','Using index' 索引覆盖查询很多情况可能会导致索引无法覆盖该查询 索引覆盖了WHERE条件中的字段，但不是整个查询涉及的字段，如下： # 使用user_id_index定位id（只覆盖user_id列），但是extra列null，所以在由聚集索引返回所有列 explain SELECT * FROM ad_account where user_id = 100; #'1','SIMPLE','ad_account',NULL,'ref','user_id_index','user_id_index','5','const','1','100.00',NULL 对于这种查询无法覆盖到所有列，可以通过子查询限定全表扫描的行数 也就是避免不了全表扫描但是可以减少扫描行（子查询返回集越大越好），如下： explain select * from ad_account a join ( SELECT id FROM ad_account where user_id = 100 )as b on a.id = b.id #'1','SIMPLE','ad_account',NULL,'ref','PRIMARY,user_id_index','user_id_index','5','const','1','100.00','Using index' #'1','SIMPLE','a',NULL,'eq_ref','PRIMARY','PRIMARY','4','ad_account.id','1','100.00',NULL "},"suo-yin/quan-wen-jian-suo.html":{"url":"suo-yin/quan-wen-jian-suo.html","title":"全文检索","keywords":"","body":"全文检索（FullTextSearch）是将存储于数据库中的整本书或整篇文章中的任意内容信息查找出来的技术。它可以根据需要获得全文中有关章、节、段、句、词等信息，也可以进行各种统计和分析 "},"suo-yin/bu-shi-yong-suo-yin-de-qing-kuang/qian-zhui-suo-yin-he-suo-yin-xuan-ze-xing.html":{"url":"suo-yin/bu-shi-yong-suo-yin-de-qing-kuang/qian-zhui-suo-yin-he-suo-yin-xuan-ze-xing.html","title":"前缀索引","keywords":"","body":" 有时候需要索引很长的字符列，这会让索引变得大且慢。 通常可以索引开始的部分字符，这样可以大大节约索引空间，从而提高索引效率。但这样也会降低索引的选择性。。对于BLOB、TEXT或者很长的VARCHAR类型的列，必须使用前缀索引，因为MySQL不允许索引这些列的完整长度。 诀窍在于要选择足够长的前缀以保证较高的选择性，同时又不能太长（以便节约空间）。前缀应该足够长，以使得前缀索引的选择性接近于索引整个列。换句话说，前缀的“基数”应该接近于完整列的“基数” --减少重复值这样索引又小又快。 #city定义7是因为它基数接近于完整列的基数， ALTER TABLE city_demo ADD KEY(city(7)); #city定义前缀是3，字符重复很多倍，增加索引值数量，降低索引效率 ALTER TABLE city_demo ADD KEY(city(3)); 有时候后缀索引（suffixindex）也有用途（例如，找到某个域名的所有电子邮件地址）。MySQL原生并不支持反向索引，但是可以把字符串反转后存储，并基于此建立前缀索引。可以通过触发器来维护这种索引。 前缀索引是一种能使索引更小、更快的有效办法，但另一方面也有其缺点：MySQL无法使用前缀索引做ORDERBY和GROUPBY，也无法使用前缀索引做覆盖扫描。 "},"suo-yin/fu-he-suo-yin.html":{"url":"suo-yin/fu-he-suo-yin.html","title":"联合索引(多列索引)","keywords":"","body":"联合索引又叫复合索引。对于复合索引:Mysql从左到右的使用索引中的字段，一个查询可以只使用索引中的一部份，但只能是最左侧部分。 索引是key index (a,b,c) 可以支持a|a,b|a,b,c 3种组合进行查找，但不支持 b,c进行查找 当最左侧字段是常量引用时，索引就十分有效,同时列之间的顺序以最少列为最左（先在最少中排除最多的原则），从本质上来说，联合索引也是一棵B+树，不同的是联合索引的键值的数量不是1，而是大于等于2。 CREATE TABLE `t` ( `a` int(11) NOT NULL, `b` int(11) DEFAULT NULL, PRIMARY KEY (`a`), KEY `idx_a_b` (`a`,`b`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8 INSERT INTO `t` VALUES (1,1),(2,2),(3,4); #使用索引 PRIMARY,idx_a_b EXPLAIN SELECT * FROM t where a = 1 and b = 1; EXPLAIN SELECT * FROM t where a = 1; #不使用索引 EXPLAIN SELECT * FROM t where b = 1; 为什么单b列查询不走索引？ 如下图可知，键值都是以a列排序的，一列a对应多列b 即（1，1）、（1，2）、（2，1）、（2，4）、（3，1）、（3，2）数据按（a，b）的a列顺序进行了存放，通过叶子节点a可以逻辑上顺序地读出所有数据 其一b列未进行排序故无法使用索引查询， 。 #上图，是在 [1,1]小于[2,4] 所以在父节点的左边 SELECT * FROM t where a = 1 and b = 1; #上图，a列进行排序了，所以是在[2,4]左边 SELECT * FROM t where a = 1 #上图，b列未排序，无法走索引定位 SELECT * FROM t where b = 1; "},"suo-yin/bu-shi-yong-suo-yin-de-qing-kuang.html":{"url":"suo-yin/bu-shi-yong-suo-yin-de-qing-kuang.html","title":"不使用索引的情况","keywords":"","body":"在某些情况下，当执行EXPLAIN命令进行SQL语句的分析时，会发现优化器并没有选择索引去查找数据，而是通过扫描聚集索引，也就是直接进行全表的扫描来得到数据。这种情况多发生于范围查找、JOIN链接操作等情况下 show index from orderdetails ; 'orderdetails','0','PRIMARY','1','orderId','A','6',NULL,NULL,'','BTREE','','' 'orderdetails','0','PRIMARY','2','productId','A','6',NULL,NULL,'','BTREE','','' 'orderdetails','1','index2','1','orderId','A','6',NULL,NULL,'','BTREE','','' #使用了聚集索引PRIMARY表扫描（索引组织表），未使用 index2 explain select orderid from orderdetails a where a.orderid > 2 and a.orderid 2 and a.orderid 2 and a.orderid 这是为什么呢？原因在于用户要选取的数据是整行信息，而index2索引不能覆盖到我们要查询的信息，因此在对index2索引查询到指定数据后，还需要一次书签访问来查找整行数据的信息。虽然index2索引中数据是顺序存放的，但是再一次进行书签查找的数据则是无序的，因此变为了磁盘上的离散读操作。如果要求访问的数据量很小，则优化器还是会选择辅助索引，但是当访问的数据占整个表中数据的蛮大一部分时（一般是20%左右），优化器会选择通过聚集索引来查找数据。因为之前已经提到过，顺序读要远远快于离散读。 "},"suo-yin/bu-shi-yong-suo-yin-de-qing-kuang/du-li-lie.html":{"url":"suo-yin/bu-shi-yong-suo-yin-de-qing-kuang/du-li-lie.html","title":"独立列","keywords":"","body":"如果查询中的列不是独立的，则MySQL就不会使用索引 “独立的列”是指索引列不能是表达式的一部分，也不能是函数的参数 索引列是表达式一部分 # 执行计划提升使用了但是，扫描84240和全表扫描一样 explain select user_id from ad_account where user_id +1 =101 #'1','SIMPLE','ad_account',NULL,'index',NULL,'user_id_index','5',NULL,'84240','100.00','Using where; Using index' #上面是等价查询结果，但是下面走索引 explain select user_id from ad_account where user_id =100 #'1','SIMPLE','ad_account',NULL,'ref','user_id_index','user_id_index','5','const','1','100.00','Using index' 函数参数 SELECT...WHERETO_DAYS(CURRENT_DATE)TO_DAYS(date_col) "},"yue-shu.html":{"url":"yue-shu.html","title":"约束","keywords":"","body":"约束该机制提供了一条强大而简易的途径来保证数据库中数据的完整性，约束和索引的概念所不同的，约束更是一个逻辑的概念，用来保证数据的完整性，而索引是一个数据结构，既有逻辑上的概念，在数据库中还代表着物理存储的方式。 InnoDB存储引擎本身提供了以下几种约束来保证数据的完整性： Primary Key（主键约束） 有两个作用，一是约束作用（constraint），用来规范一个存储主键和唯一性，但同时也在此key上建立了一个索引 Unique Key（唯一约束） 两个作用一是约束作用（constraint）规范数据的唯一性，但同时也在这个key上建立了一个index（唯一索引） Foreign Key（外健约束） 两个作用，一是约束作用（constraint），规范数据的引用完整性，但同时也在这个key上建立了一个索引 Default（默认） NOT NULL（不为空约束） "},"suo-yin-yu-suo.html":{"url":"suo-yin-yu-suo.html","title":"索引与锁","keywords":"","body":"索引可以让查询锁定更少的行。如果你的查询从不访问那些不需要的行，那么就会锁定更少的行，从两个方面来看这对性能都有好处。首先，虽然InnoDB的行锁效率很高，内存使用也很少，但是锁定行的时候仍然会带来额外开销；其次，锁定超过需要的行会增加锁争用并减少并发性。 InnoDB只有在访问行的时候才会对其加锁，而索引能够减少InnoDB访问的行数，从而减少锁的数量。但这只有当InnoDB在存储引擎层能够过滤掉所有不需要的行时才有效。如果索引无法过滤掉无效的行，那么在InnoDB检索到数据并返回给服务器层以后，MySQL服务器才能应用WHERE子句。这时已经无法避免锁定行了：InnoDB已经锁住了这些行，到适当的时候才释放。在MySQL5.1和更新的版本中，InnoDB可以在服务器端过滤掉行后就释放锁，但是在早期的MySQL版本中，InnoDB只有在事务提交后才能释放锁 "},"c_x_y_h.html":{"url":"c_x_y_h.html","title":"查询优化","keywords":"","body":" 查询sql执行过程 mysql是如何执行sql查询的。很多查询优化工作实际上就是遵循一些原则让优化器能够按照预想的合理的方式运行。 查询性能优化 在讨论查询优化之前，首先明确优化什么，对于查询优化，优化的sql查询相应时间，在mysql中sql相应时间包含两部分 阻塞时间 执行时间 其次是什么原因导致了相应时间大，其中主要的因为查询过多的行导致响应时间增大，而查询过多行包含需要的行和不需要行，通过如下方法来减少不需要行 全表扫描-可以通过添加索引来减少扫描的行 sql语句原因导致查询过多行，这包括几部分 sql语句表的关联顺序导致的(mysql关联查询使用了嵌套循环由一个驱动表开始) 以上都做了优化情况导致的原因sql查询了太多的需要数据，这种情况可以采用分而治子-切分方法减少sql执行时间，延长整个任务时间，分散压力了降低对服务器的影响,也提高缓存命中率 如何获取sql的响应时间 慢查询日志 返回行和扫描行 优化返回的行数和扫描行数之间比率，一般在1:1 或者 10:1 之间，比率越大性能越低。 索引优化 索引让MySQL以最高效、扫描行数最少的方式找到需要的记录。 库表优化 "},"sql_perform.html":{"url":"sql_perform.html","title":"mysql如何执行sql查询","keywords":"","body":"很多查询优化工作实际上就是遵循一些原则让优化器能够按照预想的合理的方式运行。 客服端发生一条查询sql 服务器先检查查询缓存是否命中，未命中继续执行下面. 服务端进行sql解析，预处理再由优化器生成执行计划 sql解析：MySQL通过关键字将SQL语句进行解析并生成一棵对应的“解析树”。 预处理器则根据一些MySQL规则进一步检查解析树是否合法， 优化器将解析树转换成执行计划(一条sql有很多执行方式，最后都返回相同的结果，优化器就是找出最好的中心计划) mysql依据执行计划调用存储引擎的API来执行查询 返回查询结果 "},"c_x_y_h/s-q-s-o.html":{"url":"c_x_y_h/s-q-s-o.html","title":"sql语句查询优化","keywords":"","body":" 优化数据访问方式,可以减少扫描的行数 如：通过explain查看该sql查询访问方式type是All全表扫描: explain SELECT name FROM nextcloud.oc_activity where info=123 ; '1', 'SIMPLE', 'oc_activity', NULL, 'ALL', NULL, NULL, NULL, NULL, '46264', '100.00', NULL 在EXPLAIN语句中的type列反应了访问类型。访问类型有很多种，从全表扫描到索引扫描、范围扫描、唯一索引查询、常数引用等。这里列的这些，速度是从慢到快，扫描的行数也是从小到大。你不需要记住这些访问类型，但需要明白扫描表、扫描索引、范围访问和单值访问的概念。如果查询没有办法找到合适的访问类型，那么解决的最好办法通常就是增加一个合适的索引 重构查询方式 切分查询:对于一个大查询需要“分而治之”，将大查询切分成小查询，每个查询功能完全一样，只完成一小部分，每次只返回一小部分查询结果。这样做的好处 如：DML语句可减少mysql复制的延迟。 事务型引擎，很多时候小事务能够更高效 。 原本一次性的压力分散到一个很长的时间段中 分解关联查询 "},"optimizer.html":{"url":"optimizer.html","title":"优化器","keywords":"","body":" 优化器是通过成本评估一条sql语句最好的执行计划 使用基于成本的选择，mysql将尝试预测一个查询使用某种执行计划时的成本，并选择其中成本最小的一个执行计划。 成本：成本的最小单位是随机读取一个4K数据页的成本，关键词last_query_cost 查看一条sql成本,如下: select sql_no_cache * from text ; show status like 'last_query_cost' ; 'Last_query_cost', '2.399000' last_query_cost 结果表示需要查找2.39个数据页才能找到查询结果结果 "},"yong-hu.html":{"url":"yong-hu.html","title":"用户管理","keywords":"","body":"MySQL修改root密码 "},"yong-hu/chuang-jian-shu-ju-he-gei-yong-hu-shou-quan.html":{"url":"yong-hu/chuang-jian-shu-ju-he-gei-yong-hu-shou-quan.html","title":"创建数据和给用户授权","keywords":"","body":"创建数据库 -- 创建数据库：txdatabase create schema txdatabase default character set utf8 collate utf8_general_ci; 采用create schema和create database创建数据库的效果一样。 创建用户 --创建用户:tx create user 'tx'@'%' identified by '123456'; 密码8位以上，包括：大写字母、小写字母、数字、特殊字符 %：匹配所有主机，还可以设置成‘localhost’，代表只能本地访问 用户授权数据库 --给用户tx授权权限与数据库 grant select,insert,update,delete,create on txdatabase.* to tx; *代表整个数据库或者指定具体表 txdatabase.aTable 立即启用修改 -- 生效修改 flush privileges ; 取消授权数据库 --或者指定数据库 revoke all on *txdatabase* from tx; --取消用户tx所有数据库（表）的所有权限 revoke all on *.* from tx; 删除 --删除用户 delete from mysql.user where user='tx'; --删除数据库 drop database schema txdatabase; "},"yong-hu/xiu-gai-root-mi-ma.html":{"url":"yong-hu/xiu-gai-root-mi-ma.html","title":"修改root密码","keywords":"","body":"升级的mysql5.7修改完root账户密码后仍然无法登陆，查阅资料可能和user表的plugin 字段为空有关。 1、首先将my.ini中加入在[mysqld]节点上加skip-grant-tables 主要作用是：跳过表中的验证,可以无密码登陆。 2、登录之后查询plugin字段值： mysql> select plugin from user where user = 'root'; 执行结果plugin字段为空。 3、更新plugin字段为mysql默认值： mysql> update user set plugin='mysql_native_password'; 查询更新结果： 4、更新成功，继续执行更新密码操作： mysql> update user set authentication_string=password('123456') where user='root' and host='localhost'; 5、刷新权限： mysql> flush privileges; 6、将my.ini中的skip-grant-tables注释掉或者删掉 "},"suo-yin/shu.html":{"url":"suo-yin/shu.html","title":"树(tree)","keywords":"","body":"树（Tree）是n（n>=0)个结点的有限集。n=0时称为空树。在任意一颗非空树中： 有且仅有一个特定的称为根（Root）的结点； 当n>1时，其余结点可分为m(m>0)个互不相交的有限集T1、T2、......、Tn，其中每一个有限集合本身又是一棵树，并且称为根的子树。 此外，树的定义还需要强调以下两点： n>0时根结点是唯一的，不可能存在多个根结点，数据结构中的树只能有一个根结点。 m>0时，子树的个数没有限制，但它们一定是互不相交的。 graph TD a((根结点)); b((a子结点)); c((b子结点)); d((c子结点)); d1((子结点)); d2((子结点)); d3((子结点)); b1((子结点)); b2((子结点)); b3((子结点)); a-->b ; a-->c ; a-->d ; d-->d1 ; d-->d2 ; d-->d3 ; b-->b1 ; b-->b2 ; b-->b3 ; 由树的结构可以看出，树的定义使用了递归的方式。 结点的度 是结点拥有的子树数目称为结点的度。 如上图根结拥有3个子结点，所以高度是3 ，b子结点为0 a子结点3 结点之间关系 双亲结点:根结点是a,b,c的双亲结点 孩子结点:a,b,c是根结点孩子结点 兄弟结点:a,b,c是兄弟结点 结点深度或高度 树中结点所处的最大层数称为树的高度，如空树的高度为0，只有一个根结点的树高度为1,从根开始定义起，根为第一层，根的孩子为第二层，以此类推 ，同时树中结点的最大层次数称为树的深度或高度 如上图为3层深度也是3 "},"suo-yin/er-cha-shu.html":{"url":"suo-yin/er-cha-shu.html","title":"二叉树","keywords":"","body":"在树形结构中，它的每个结点最多只有两棵子树，这种树被称为二叉树，也称为有序树，树结构是否无序。 二叉树是n(n>=0)个结点的有限集合，该集合或者为空集（n=0称为空二叉树），或者由一个根结点和两棵互不相交的子结点集组成、分别称为根结点的左子树和右子树组成。 graph TD a((根结点)); b((a子结点)); c((b子结点)); b1((子结点)); b2((子结点)); a-->b ; a-->c ; b-->b1 ; b-->b2 ; 由二叉树定义以及图示分析得出二叉树有以下特点： 每个结点最多有两颗子树，所以二叉树中不存在度大于2的结点。 左子树和右子树是有顺序的，次序不能任意颠倒。 即使树中某结点只有一棵子树，也要区分它是左子树还是右子树。 二叉树性质 在二叉树的第i层上最多有2i-1 个节点（i>=1） 。 二叉树中如果深度为k,那么最多有2k-1个节点(k>=1）。 n0=n2+1 n0表示度数为0的节点数，n2表示度数为2的节点数。 在完全二叉树中，具有n个节点的完全二叉树的深度为[log2n]+1，其中[log2n]是向下取整。 若对含 n 个结点的完全二叉树从上到下且从左至右进行 1 至 n 的编号，则对完全二叉树中任意一个编号为 i 的结点有如下特性： 二叉树的五种不同的形态： 空二叉 只有根结点 只有左子树 只有右子树 左右子树均非空 两种特殊的二叉树： 满二叉树 完全二叉树 满二叉树的叶子结点全部在最底层，而完全二叉树的叶子结点可以分布在最下面两层（最下层和次下层）， "},"suo-yin/man-er-cha-shu.html":{"url":"suo-yin/man-er-cha-shu.html","title":"满二叉树","keywords":"","body":"满二叉树 在一棵二叉树中,如果所有分支结点都存在左子树和右子树，并且所有叶子都在同一层上，这样的二叉树称为满二叉树。 满二叉树的特点有： 叶子只能出现在最下一层。出现在其它层就不可能达成平衡。 非叶子结点的度一定是2。 在同样深度的二叉树中，满二叉树的结点个数最多，叶子数最多。 叶子结点是离散数学中的概念。一棵树当中没有子结点（即度为0）的结点称为叶子结点，简称“叶子”。 叶子是指度为0的结点，又称为终端结点。 graph TD a((根结点)); b((左子树)); c((右子树)); b1((叶子结点)); b2((叶子结点)); c1((叶子结点)); c2((叶子结点)); a-->b ; a-->c ; b-->b1 ; b-->b2 ; c-->c1 ; c-->c2; "},"suo-yin/wan-quan-er-cha-shu.html":{"url":"suo-yin/wan-quan-er-cha-shu.html","title":"完全二叉树","keywords":"","body":"完全二叉树 对一颗具有n个结点的二叉树按层编号，如果编号为i(1 叶子结点只能出现在最下层和次下层。 最下层的叶子结点集中在树的左部。 倒数第二层若存在叶子结点，一定在右部连续位置。 如果结点度为1，则该结点只有左孩子，即没有右子树。 同样结点数目的二叉树，完全二叉树深度最小。 注：满二叉树一定是完全二叉树，但反过来不一定成立。完全二叉一定是一棵平衡二叉树，但反过来不一定成立。 "},"suo-yin/dong-tai-cha-zhao-shu.html":{"url":"suo-yin/dong-tai-cha-zhao-shu.html","title":"树的查找法","keywords":"","body":"树表查找法又称为基于树的查找法，是将待查表组织成特定树的形式并在树结构上实现查找的方法，主要包括 二叉排序树（Binary Search Tree），平衡二叉查找树（Balanced Binary Search Tree），B-tree树。 前三者是典型的二叉查找树结构，其查找的时间复杂度O(log2N)与树的深度相关，那么降低树的深度自然会提高查找效率。 二叉排序树（BinarySortingTree）又称二叉搜索树或二叉查找树，它或者是一棵空树，或者是一棵具有如下特征的非空二叉树： 若它的左子树非空，则左子树上所有结点的关键字均小于根结点的关键字。 若它的右子树非空，则右子树上所有结点的关键字均大于等于根结点的关键字。 左、右子树本身又都是一棵二叉排序树。 平衡二叉树 当要求查找的性能较高时，要对构成二叉排序树的过程进行“平衡化”处理，形成平衡二叉树。 若一棵二叉树中每个结点的左、右子树的深度之差的绝对值不超过1，则称这样的二叉树为平衡二叉树 平衡因子（BalanceFactor）将该结点的左子树深度减去右子树深度的值，称为该结点的平衡因子。也就是说，一棵二叉排序树中，所有结点的平衡因子只能为0、1、1时，则该二叉排序树就是一棵平衡二叉树 B-tree树 "},"suo-yin/bshu-ff08-b-tree.html":{"url":"suo-yin/bshu-ff08-b-tree.html","title":"B树(B-tree)","keywords":"","body":"B-树就是B树，而不要读成B减树，它不像传统的树那样，每个结点只含有一个数据元素，而是可含多个元素，是一种平衡的多叉树，称为多路平衡查找树（或m路平衡查找树） 在文件组织中，树形结构通常是作为索引文件（索引表和数据区）的索引表结构。索引文件的树形索引表结构通常采用B树结构（尤其是B+）。B+树是应文件系统所需而出的一种B树的变形树， 一棵m阶的B树（或m路查找树）是一棵空树，或者是满足如下性质的树 除根结点和叶子结点之外，其余每个结点至少有「m/2（取不小于m/2的最小整数）个孩子，至多有m个孩子（或m棵子树，有m-1个关键字) 。 所有的叶子结点都在同一层上且不包含任何信息。（可以把叶结点看成外部结点或查找失败时的结点，实际上这些结点不存在（虚结点），指向这些结点的指针为空。引入失败结点是为了便于分析B树的查找性能） 根结点至少有2个孩子，至多有m个孩子 。 所有的非叶子结点中包含下列信息（n，A0，K1，A1，K2，A2，…，Kn，An）。（关键字从小到大）。 其中，n代表关键字个数，K1，K2，…，Kn为n个从小到大排列的关键字，A0，A1，…，An为n+1个指针，其中A0所指向孩子中的所有关键字均小于,K1，A1所指向孩子中的所有关键字均大于K1，小于K2，依次下去，An所指向孩子中的所有关键字均大于Kn。 一棵m(m=5)阶的B树 查找-71 从根结点开始，找到结点a，由于a中只有一个关键字，且71＞54，由A1可找到结点C，有两个关键字（69，78）且69＜71＜78，若71存在，则必在指针A1所指的孩子内，由指针A1找到结点h，在结点h中顺序检索，可检索到关键字71。此时检索成功。 一棵m(m=4)阶的B树 由以上两例可知： 根至少有两棵子树。 4阶B树每个结点至少有2棵子树，1个关键字，最多有4棵子树，3个关键字；5阶B树每个结点至少有3棵子树，2个关键字，最多有5棵子树，4个关键字。 叶子结点出现在同一层上。 B树的插入（B树由底向上生长） 与构造二叉检索树不同的是，在B树中不是添加新的叶子结点，而是首先在最低层的某个非终端结点中添加一个关键字(判断该结点是否已有m1个关键字，若不是m1，则按关键字的大小有序地插入到适当位置，否则由于结点的关键字个数为m，超过规定的范围，需进行结点的“分裂”) 向一颗2阶段空B树插入1 graph TD 1(1); 插入2关键字，该节点关键字数量>=m了，需要分裂(keys[关键字数量/2] 提升成父节点) graph TD 1(1); 2(2); 2-->1 ; 插入4关键字 graph TD 1(1); 2(2); 4(4); 2-->1 ; 2-->4 ; 插入6关键字 graph TD 1(1); 2(2); 4(4,6); 6(6); 2-->1 ; 2-->4 ; * 该节点关键字数量>=m了，需要分裂,首先将6提升到父节点关键字中 graph TD 1(1); 2(2); 4(4); 6(6); 6-->2 ; 2-->1 ; 2-->4 ; 插入7关键字 graph TD 1(1); 2(2); 4(4); 6(6); 7(7); 6-->2; 6-->7; 2-->1; 2-->4; 插入3关键字 graph TD 1(1); 2(2); 4(3,4); 6(6); 7(7); 6-->2; 6-->7; 2-->1; 2-->4; 该节点关键字数量>=m了，需要分裂 graph TD 1(1); 2(2,3); 4(4); 6(6); 7(7); 6-->2; 6-->7; 2-->1; 2-->4; 该节点关键字数量>=m了，需要分裂 graph TD 1(1); 2(2); 4(4); 6(6,3); 7(7); 6-->2; 6-->7; 2-->1; 2-->4; 该节点关键字数量>=m了，需要分裂 graph TD 1(1); 2(2); 4(4); 6(6); 7(7); 3(3); 3-->6; 6-->2; 6-->7; 2-->1; 2-->4; B树删除 必须依据以下三种情形进行处理： 若删除后该点的关键字数目仍大于或等于1，则直接将其删除。 若删除后该结点的关键字数目小于1，而与该结点相邻的右（左）兄弟结点中的关键字数目大于1，此时，需将其右（左）兄弟中最小（最大）的关键字移到双亲结点中，而将双亲结点中小于（大于）该上移关键字的关键字下移到被删除关键字所在结点中。 若删除后该结点的关键字数目小于1，而与该结点相邻的右（左）兄弟结点中的关键字数目等于1，就合法调剂，进行结点的合并，将双亲结点中指向该结点的一个关键字合并到右（左）兄弟中，再删除该结点。 "},"suo-yin/bshu-ff08-b-tree/java.html":{"url":"suo-yin/bshu-ff08-b-tree/java.html","title":"java","keywords":"","body":" public class BPlusTree> { private Integer bTreeOrder; private Integer maxNumber; private Node root; private LeafNode left; public BPlusTree() { this(3); } public BPlusTree(Integer bTreeOrder) { this.bTreeOrder = bTreeOrder; this.maxNumber = bTreeOrder + 1; this.root = new LeafNode(); this.left = null; } public T find(V key) { T t = this.root.find(key); if (t == null) { System.out.println(\"找不到:\" + key); } return t; } public void insert(T value, V key) { if (key == null) return; Node t = this.root.insert(value, key); if (t != null) this.root = t; this.left = (LeafNode) this.root.refreshLeft(); } abstract class Node> { protected Node parent; protected Node[] childs; protected Integer number; protected Object[] keys; public Node() { this.keys = new Object[maxNumber]; this.childs = new Node[maxNumber]; this.number = 0; this.parent = null; } abstract T find(V key); abstract Node insert(T value, V key); abstract LeafNode refreshLeft(); } class BPlusNode> extends Node { public BPlusNode() { super(); } @Override T find(V key) { int i = 0; while (i insert(T value, V key) { int i = 0; while (i 0) i--; return this.childs[i].insert(value, key); } @Override LeafNode refreshLeft() { return this.childs[0].refreshLeft(); } Node insertNode(Node node1, Node node2, V key) { V oldKey = null; if (this.number > 0) oldKey = (V) this.keys[this.number - 1]; if (key == null || this.number tempNode = new BPlusNode(); tempNode.number = this.number - middle; tempNode.parent = this.parent; if (this.parent == null) { BPlusNode tempBPlusNode = new BPlusNode<>(); tempNode.parent = tempBPlusNode; this.parent = tempBPlusNode; oldKey = null; } System.arraycopy(tempKeys, middle, tempNode.keys, 0, tempNode.number); System.arraycopy(tempChilds, middle, tempNode.childs, 0, tempNode.number); for (int j = 0; j parentNode = (BPlusNode) this.parent; return parentNode.insertNode(this, tempNode, oldKey); } } class LeafNode> extends Node { protected Object[] values; protected LeafNode leftNode; protected LeafNode rightNode; public LeafNode() { super(); this.values = new Object[maxNumber]; this.leftNode = null; this.rightNode = null; } @Override T find(V key) { if (this.number insert(T value, V key) { V oldKey = null; if (this.number > 0) oldKey = (V) this.keys[this.number - 1]; int i = 0; while (i 0) { node.parent.keys[node.parent.number - 1] = tempKey; node = node.parent; } } return null; } Integer middle = this.number / 2; LeafNode tempNode = new LeafNode(); tempNode.number = this.number - middle; tempNode.parent = this.parent; if (this.parent == null) { BPlusNode tempBPlusNode = new BPlusNode<>(); tempNode.parent = tempBPlusNode; this.parent = tempBPlusNode; oldKey = null; } //{1,2,3} System.arraycopy(tempKeys, middle, tempNode.keys, 0, tempNode.number); System.arraycopy(tempValues, middle, tempNode.values, 0, tempNode.number); this.number = middle; this.keys = new Object[maxNumber]; this.values = new Object[maxNumber]; System.arraycopy(tempKeys, 0, this.keys, 0, middle); System.arraycopy(tempValues, 0, this.values, 0, middle); this.rightNode = tempNode; tempNode.leftNode = this; BPlusNode parentNode = (BPlusNode) this.parent; return parentNode.insertNode(this, tempNode, oldKey); } @Override LeafNode refreshLeft() { if (this.number "},"suo-yin/b681128-b-+-tree.html":{"url":"suo-yin/b681128-b-+-tree.html","title":"B+树(B+tree)","keywords":"","body":"B+树是应文件系统所需而产生的一种B树的变形树，也是一种多路搜索树。一棵m阶的B+树和m阶的B树定义基本相同，差异如下。 非叶子结点的指针数与关键字个数相同；有n棵子树的结点中含有n个关键码。 所有的叶子结点中包含了全部关键码的信息，以及指向含有这些关键码记录的指针，且叶子结点本身依关键码的大小按从小到大的顺序链接。 所有的非终端结点可以看成是索引部分，结点中仅含有其子树根结点中最大（或最小）关键码。 一棵4阶的B+树 通常在B+树上有两个头指针，一个指向根结点，另一个指向关键码最小的叶子结点,因此，可以对B+树进行两种查找运算：一种是从最小关键码开始顺序查找，另一种是从根结点开始，进行随机查找 B+的特性如下 所有关键字都出现在叶子结点的链表中（稠密索引），且链表中的关键字恰好是有序的。 关键字的查找不可能在非叶子结点命中 非叶子结点相当于叶子结点的索引（稀疏索引），叶子结点相当于存储（关键字）数据的数据层。 更适合文件索引系统。 原因有两点：相对于B树，B+树空间利用率更高，因为B+树的内部结点只是作为索引使用，而不像B树那样每个结点都需要存储硬盘指针。增删文件（结点）时，效率更高，因为B+树的叶子结点包含所有关键字，并以有序的链表结构存储，这样可提高增删效率。 但是B+索引在数据库中有一个特点是高扇出性，因此在数据库中，B+树的高度一般都在2~4层，这也就是说查找某一键值的行记录时最多只需要2到4次IO，这倒不错。因为当前一般的机械磁盘每秒至少可以做100次IO，2~4次的IO意味着查询时间只需0.02~0.04秒 InnoDB存储引擎表是索引组织表，即表中数据按照主键顺序存放。而聚集索引就是按照每张表的主键构造一棵B+树，同时叶子节点中存放的即为整张表的行记录数据，也将聚集索引的叶子节点称为数据页 "},"suo-yin/er-cha-pai-xu-shu.html":{"url":"suo-yin/er-cha-pai-xu-shu.html","title":"二叉排序树","keywords":"","body":" /** * * 二叉排序树实现 * @param */ public class SortedBinTree { static class Node { Object data; Node parent; Node left; Node right; public Node(Object data, Node parent, Node left, Node right) { this.data = data; this.parent = parent; this.left = left; this.right = right; } public String toString() { return \"[data=\" + data + \"]\"; } public boolean equals(Object obj) { if (this == obj) { return true; } if (obj.getClass() == Node.class) { Node target = (Node) obj; return data.equals(target.data) && left == target.left && right == target.right && parent == target.parent; } return false; } } private Node root; // 两个构造器用于创建排序二叉树 public SortedBinTree() { root = null; } public SortedBinTree(T o) { root = new Node(o, null, null, null); } // 添加节点 public void add(T ele) { // 如果根节点为null if (root == null) { root = new Node(ele, null, null, null); } else { Node current = root; Node parent = null; int cmp = 0; // 搜索合适的叶子节点，以该叶子节点为父节点添加新节点 do { parent = current; cmp = ele.compareTo(current.data); // 如果新节点的值大于当前节点的值 if (cmp > 0) { // 以右子节点作为当前节点 current = current.right; } else { // 如果新节点的值小于当前节点的值 // 以左节点作为当前节点 current = current.left; } } while (current != null); // 创建新节点 Node newNode = new Node(ele, parent, null, null); // 如果新节点的值大于父节点的值 if (cmp > 0) { // 新节点作为父节点的右子节点 parent.right = newNode; } else { // 如果新节点的值小于父节点的值 // 新节点作为父节点的左子节点 parent.left = newNode; } } } // 删除节点 public void remove(T ele) { // 获取要删除的节点 Node target = getNode(ele); if (target == null) { return; } // 左、右子树为空 if (target.left == null && target.right == null) { // 被删除节点是根节点 if (target == root) { root = null; } else { // 被删除节点是父节点的左子节点 if (target == target.parent.left) { // 将target的父节点的left设为null target.parent.left = null; } else { // 将target的父节点的right设为null target.parent.right = null; } target.parent = null; } } else if (target.left == null && target.right != null) { // 左子树为空，右子树不为空 // 被删除节点是根节点 if (target == root) { root = target.right; } else { // 被删除节点是父节点的左子节点 if (target == target.parent.left) { // 让target的父节点的left指向target的右子树 target.parent.left = target.right; } else { // 让target的父节点的right指向target的右子树 target.parent.right = target.right; } // 让target的右子树的parent指向target的parent target.right.parent = target.parent; } } else if (target.left != null && target.right == null) { // 左子树不为空，右子树为空 // 被删除节点是根节点 if (target == root) { root = target.left; } else { // 被删除节点是父节点的左子节点 if (target == target.parent.left) { // 让target的父节点的left指向target的左子树 target.parent.left = target.left; } else { // 让target的父节点的right指向target的左子树 target.parent.right = target.left; } // 让target的左子树的parent指向target的parent target.left.parent = target.parent; } } else { // 左、右子树都不为空 // leftMaxNode用于保存target节点的左子树中值最大的节点 Node leftMaxNode = target.left; // 搜索target节点的左子树中值最大的节点 while (leftMaxNode.right != null) { leftMaxNode = leftMaxNode.right; } // 从原来的子树中删除leftMaxNode节点 leftMaxNode.parent.right = null; // 让leftMaxNode的parent指向target的parent leftMaxNode.parent = target.parent; // 被删除节点是父节点的左子节点 if (target == target.parent.left) { // 让target的父节点的left指向leftMaxNode target.parent.left = leftMaxNode; } else { // 让target的父节点的right指向leftMaxNode target.parent.right = leftMaxNode; } leftMaxNode.left = target.left; leftMaxNode.right = target.right; target.parent = target.left = target.right = null; } } // 根据给定的值搜索节点 public Node getNode(T ele) { // 从根节点开始搜索 Node p = root; while (p != null) { int cmp = ele.compareTo(p.data); // 如果搜索的值小于当前p节点的值 if (cmp 0) { // 如果搜索的值大于当前p节点的值 // 向右子树搜索 p = p.right; } else { return p; } } return null; } // 广度优先遍历 public List breadthFirst() { Queue queue = new ArrayDeque(); List list = new ArrayList(); if (root != null) { // 将根元素入“队列” queue.offer(root); } while (!queue.isEmpty()) { // 将该队列的“队尾”的元素添加到List中 list.add(queue.peek()); Node p = queue.poll(); // 如果左子节点不为null，将它加入“队列” if (p.left != null) { queue.offer(p.left); } // 如果右子节点不为null，将它加入“队列” if (p.right != null) { queue.offer(p.right); } } return list; } public void treeString(StringBuffer stringBuffer,Node node) { if (node == null) return; stringBuffer.append(node.data).append(\"((\").append(node.data).append(\")); \\n\") ; if (node.parent != null ){ stringBuffer.append(node.parent.data).append(\"-->\").append(node.data).append(\"; \\n\") ; } if (node.left != null ){ treeString(stringBuffer,node.left) ; } if (node.right != null) { treeString(stringBuffer,node.right) ; } } public static void main(String[] args) { SortedBinTree tree = new SortedBinTree(); // 添加节点 tree.add(5); tree.add(20); tree.add(10); tree.add(3); tree.add(8); tree.add(15); tree.add(30); tree.add(31); tree.add(21); StringBuffer stringBuffer = new StringBuffer() ; System.out.println(tree.treeString(stringBuffer,tree.root)); // 删除节点 //tree.remove(20); //System.out.println(tree.breadthFirst()); } } 图下： graph TD 5((5)); 3((3)); 5-->3; 20((20)); 5-->20; 10((10)); 20-->10; 8((8)); 10-->8; 15((15)); 10-->15; 30((30)); 20-->30; 21((21)); 30-->21; 31((31)); 30-->31; "},"suo-yin/ping-heng-er-cha-shu.html":{"url":"suo-yin/ping-heng-er-cha-shu.html","title":"平衡二叉树","keywords":"","body":"平衡二叉树 若一棵二叉排序树中每个结点的左、右子树的深度之差的绝对值不超过1，则称这样的二叉树为平衡二叉树。 为了降低二叉排序树的高度，提高查找效率 平衡因子（BalanceFactor） 将该结点的左子树深度减去右子树深度的值，称为该结点的平衡因子。也就是说，一棵二叉排序树中，所有结点的平衡因子只能为0、1、-1时，则该二叉排序树就是一棵平衡二叉树 如图是一棵平衡二叉树： graph TD 5((5)); 2((2)); 6((6)); 3((3)); 4((4)); 7((7)); 1((1)); 5 --> 2 ; 5 --> 6 ; 6 --> 7 ; 2 --> 1 ; 2 --> 3 ; 3 --> 4 ; 根节点5的左右子树的深度差是1 节点2的左子树共2层，右子树3层，该节点的平衡因子=-1 节点3的左子树0层，右子树1层，该节点的平衡因子=1 节点6的左子树0层，右子树1层，该节点的平衡因子=1 剩余是叶子节点均是0 节点的度为0称为叶子节点，也就是该节点没有子节点的节点称为叶子节点 平衡处理 插入某个结点后，可能会变成非平衡二叉树，这时，就可以对该二叉树进行平衡处理。 如下是一棵二叉排序树，进行平衡处理，处理的原则应该是处理与插入点最近的平衡因子又比1大或比1小的结点 graph TD 5((5)); 3((3)); 5-->3; 20((20)); 5-->20; 10((10)); 20-->10; 8((8)); 10-->8; 15((15)); 10-->15; 30((30)); 20-->30; 21((21)); 30-->21; 31((31)); 30-->31; 先对20节点进行平衡处理，使其每个节点的平衡因子都是 1 0 -1 中 graph TD 5((5)); 3((3)); 8-->3; 20((20)); 8-->5; 10((10)); 20-->10; 8((8)); 10-->8; 15((15)); 10-->15; 30((30)); 20-->30; 21((21)); 30-->21; 31((31)); 30-->31; 4种平衡处理方法 左左型(LL)：在C的左孩子B上插入一个左孩子结点A，使C的平衡因子由1变成了2，成为不平衡的二叉树序树 平衡处理：将C顺时针旋转，成为B的右子树，而原来B的右子树则变成C的左子树，待插入结点A作为B的左子树 右右型(RR)：在A的右孩子B上插入一个右孩子C，使A的平衡因子由1变成2 平衡处理：将A逆时针旋转，成为B的左子树，而原来B的左子树则变成A的右子树，待插入结点C成为B的右子树。 左右型(LR)：在C的左孩子A上插入一个右孩子B，使的C的平衡因子由1变成2 平衡处理：将B变到A与C之间，使之成为LL型，然后按LL型处理 右左型(RL)：在A的右孩子C上插入一个左孩子B，使A的平衡因子由-1变成-2 平衡处理：将B变到A与C之间，使之成为RR型，然后按RR型处理 "},"dao-5165-dao-chu.html":{"url":"dao-5165-dao-chu.html","title":"导入/导出","keywords":"","body":" 数据库备份数据-指定编码 mysqldump -uroot -p --default-character-set=gbk dbname > /root/newsdata.sql 或 mysqldump -uroot -p --default-character-set=utf8 dbname > /root/newsdata.sql 导入数据库-设置编码 mysql -uroot -p --default-character-set=gbk use dbname source /root/newsdata.sql 或 mysql -uroot -p --default-character-set=utf8 use dbname source /root/newsdata.sql mysql数据库设置编码命令 SET character_set_client = utf8; SET character_set_connection = utf8; SET character_set_database = utf8; SET character_set_results = utf8;/*这里要注意很有用*/ SET character_set_server = utf8; SET collation_connection = utf8_bin; SET collation_database = utf8_bin; SET collation_server = utf8_bin; my.ini中配置默认编码 default-character-set=utf8 "},"qiang-zhi-xing-cao-zuo.html":{"url":"qiang-zhi-xing-cao-zuo.html","title":"强制性操作","keywords":"","body":"mysql常用的hint 对于经常使用oracle的朋友可能知道，oracle的hint功能种类很多，对于优化sql语句提供了很多方法。同样，在mysql里，也有类似的hint功能。下面介绍一些常用的。 强制索引 FORCE INDEX SELECT * FROM TABLE1 FORCE INDEX (FIELD1) … 以上的SQL语句只使用建立在FIELD1上的索引，而不使用其它字段上的索引。 忽略索引 IGNORE INDEX SELECT * FROM TABLE1 IGNORE INDEX (FIELD1, FIELD2) … 在上面的SQL语句中，TABLE1表中FIELD1和FIELD2上的索引不被使用。 关闭查询缓冲 SQL_NO_CACHE SELECT SQL_NO_CACHE field1, field2 FROM TABLE1; 有一些SQL语句需要实时地查询数据，或者并不经常使用(可能一天就执行一两次),这样就需要把缓冲关了,不管这条SQL语句是否被执行过，服务器都不会在缓冲区中查找，每次都会执行它。 强制查询缓冲 SQL_CACHE SELECT SQL_CALHE * FROM TABLE1; 如果在my.ini中的query_cache_type设成2，这样只有在使用了SQL_CACHE后，才使用查询缓冲。 优先操作 HIGH_PRIORITY HIGH_PRIORITY可以使用在select和insert操作中，让MYSQL知道，这个操作优先进行。 SELECT HIGH_PRIORITY FROM TABLE1; *滞后操作 LOW_PRIORITY LOW_PRIORITY可以使用在insert和update操作中，让mysql知道，这个操作滞后。 update LOW_PRIORITY table1 set field1= where field1= … 延时插入 INSERT DELAYED INSERT DELAYED INTO table1 set field1= … INSERT DELAYED INTO，是客户端提交数据给MySQL，MySQL返回OK状态给客户端。而这是并不是已经将数据插入表，而是存储在内存里面等待排队。当mysql有空余时，再插入。另一个重要的好处是，来自许多客户端的插入被集中在一起，并被编写入一个块。这比执行许多独立的插入要快很多。坏处是，不能返回自动递增的ID，以及系统崩溃时，MySQL还没有来得及插入数据的话，这些数据将会丢失。 强制连接顺序 STRAIGHT_JOIN SELECT TABLE1.FIELD1, TABLE2.FIELD2 FROM TABLE1 STRAIGHT_JOIN TABLE2 WHERE … 由上面的SQL语句可知，通过STRAIGHT_JOIN强迫MySQL按TABLE1、TABLE2的顺序连接表。如果你认为按自己的顺序比MySQL推荐的顺序进行连接的效率高的话，就可以通过STRAIGHT_JOIN来确定连接顺序。 强制使用临时表 SQL_BUFFER_RESULT SELECT SQL_BUFFER_RESULT * FROM TABLE1 WHERE … 当我们查询的结果集中的数据比较多时，可以通过SQL_BUFFER_RESULT.选项强制将结果集放到临时表中，这样就可以很快地释放MySQL的表锁(这样其它的SQL语句就可以对这些记录进行查询了)，并且可以长时间地为客户端提供大记录集。 分组使用临时表 SQL_BIG_RESULT和SQL_SMALL_RESULT SELECT SQL_BUFFER_RESULT FIELD1, COUNT(*) FROM TABLE1 GROUP BY FIELD1; 一般用于分组或DISTINCT关键字，这个选项通知MySQL，如果有必要，就将查询结果放到临时表中，甚至在临时表中进行排序。SQL_SMALL_RESULT比起SQL_BIG_RESULT差不多，很少使用。 "},"sqlyu-yan-de-fen-lei.html":{"url":"sqlyu-yan-de-fen-lei.html","title":"SQL语言的分类","keywords":"","body":"SQL语言的分类为四大类：数据查询语言DQL，数据操纵语言DML，数据定义语言DDL，数据控制语言DCL 数据查询语言DQL 数据查询语言DQL基本结构是由SELECT子句，FROM子句，WHERE 子句组成的查询块： SELECT FROM WHERE 2 .数据操纵语言DML 数据操纵语言DML主要有三种形式： 1) 插入：INSERT 2) 更新：UPDATE 3) 删除：DELETE 数据定义语言DDL 数据定义语言DDL用来创建数据库中的各种对象-----表、视图、 索引、同义词、聚簇等如： CREATE TABLE/VIEW/INDEX/SYN/CLUSTER 表 视图 索引 同义词 簇 DDL操作是隐性提交的！不能rollback 数据控制语言DCL 数据控制语言DCL用来授予或回收访问数据库的某种特权，并控制 数据库操纵事务发生的时间及效果，对数据库实行监视等。如： 1) GRANT：授权。 2) ROLLBACK [WORK] TO [SAVEPOINT]：回退到某一点。 回滚---ROLLBACK 回滚命令使数据库状态回到上次最后提交的状态。其格式为： SQL>ROLLBACK; 3) COMMIT [WORK]：提交。 在数据库的插入、删除和修改操作时，只有当事务在提交到数据 库时才算完成。在事务提交前，只有操作数据库的这个人才能有权看 到所做的事情，别人只有在最后提交完成后才可以看到。 提交数据有三种类型：显式提交、隐式提交及自动提交。下面分 别说明这三种类型。 (1) 显式提交 用COMMIT命令直接完成的提交为显式提交。其格式为： SQL>COMMIT； (2) 隐式提交 用SQL命令间接完成的提交为隐式提交。这些命令是： ALTER，AUDIT，COMMENT，CONNECT，CREATE，DISCONNECT，DROP， EXIT，GRANT，NOAUDIT，QUIT，REVOKE，RENAME。 (3) 自动提交 若把AUTOCOMMIT设置为ON，则在插入、修改、删除语句执行后， 系统将自动进行提交，这就是自动提交。其格式为： SQL>SET AUTOCOMMIT ON； "},"hint-jian-jie.html":{"url":"hint-jian-jie.html","title":"Hint 简介","keywords":"","body":"HINT 作为一种 SQL 补充语法，在关系型数据库中扮演着非常重要的角色。它允许用户通过相关的语法影响 SQL 的执行方式，对 SQL 进行特殊的优化。同样，DRDS 也提供了特殊的 HINT 语法。 例如，假设已知目标数据在某些分库的分表中，需要直接将 SQL 下发到该分库执行，就可以使用 DRDS 自定义 HINT 来完成。 SELECT /*+TDDL:node('node_name')*/ * FROM table_name; 这个 SQL 语句中/和/之间的语句就是 DRDS 的自定义 HINT，即+TDDL:node('node_name')，它指定了 SQL 语句在特定的 RDS 分库上执行。 注意： DRDS 自定义 HINT 支持 /*+TDDL:hint_command*/ 和 /!+TDDL:hint_command*/ 两种格式。 如果使用 /*+TDDL:hint_command*/ 格式，在使用 MySQL 官方命令行客户端执行带有 DRDS 自定义 HINT 的 SQL 时，请在登录命令中加上 -c 参数。否则，由于 DRDS 自定义 HINT 是以 MySQL 注释 形式使用的，该客户端会将注释语句删除后再发送到服务端执行，导致 DRDS 自定义 HINT 失效。具体请查看 MySQL 官方客户端命令。 DRDS 自定义 HINT 语法 基本语法： /*+TDDL: hint_command [hint_command ...]*/ /!+TDDL: hint_command [hint_command ...]*/ DRDS 自定义 HINT 基于MySQL 注释，HINT 语句位于 /与/ 或 /!与*/ 之间，并且必须以+TDDL:开头。其中 hint_command 是 DRDS 自定义 HINT 命令，与具体的操作相关, 多个 hint_command 之间使用空格分割。 例子： # 查询每个分库中的物理表名 /*+TDDL:scan()*/SHOW TABLES; # 将查询下发到 RDS 只读实例的 0000 分库上 /*+TDDL:node(0) slave()*/SELECT * FROM t1; 例子中 /+TDDL:scan()/ 和 /+TDDL:node(0) slave()/ 为 DRDS 自定义 HINT 部分，以+TDDL:开头。scan()、node(0)、slave() 为 DRDS 自定义 HINT 命令，多个 HINT 命令之间使用空格分割。 "},"hint-jian-jie/mysqlduihint-de-zhi-chi.html":{"url":"hint-jian-jie/mysqlduihint-de-zhi-chi.html","title":"Mysql对Hint的支持","keywords":"","body":"mysql常用的hint 对于经常使用oracle的朋友可能知道，oracle的hint功能种类很多，对于优化sql语句提供了很多方法。同样，在mysql里，也有类似的hint功能。下面介绍一些常用的。 强制索引 FORCE INDEX SELECT FROM TABLE1 FORCE INDEX (FIELD1) … 以上的SQL语句只使用建立在FIELD1上的索引，而不使用其它字段上的索引。 忽略索引 IGNORE INDEX SELECT FROM TABLE1 IGNORE INDEX (FIELD1, FIELD2) … 在上面的SQL语句中，TABLE1表中FIELD1和FIELD2上的索引不被使用。 关闭查询缓冲 SQL_NO_CACHE SELECT SQL_NO_CACHE field1, field2 FROM TABLE1; 有一些SQL语句需要实时地查询数据，或者并不经常使用（可能一天就执行一两次）,这样就需要把缓冲关了,不管这条SQL语句是否被执行过，服务器都不会在缓冲区中查找，每次都会执行它。 强制查询缓冲 SQL_CACHE SELECT SQL_CALHE * FROM TABLE1; 如果在my.ini中的query_cache_type设成2，这样只有在使用了SQL_CACHE后，才使用查询缓冲。 优先操作 HIGH_PRIORITY HIGH_PRIORITY可以使用在select和insert操作中，让MYSQL知道，这个操作优先进行。 SELECT HIGH_PRIORITY * FROM TABLE1; 滞后操作 LOW_PRIORITY LOW_PRIORITY可以使用在insert和update操作中，让mysql知道，这个操作滞后。 update LOW_PRIORITY table1 set field1= where field1= … 延时插入 INSERT DELAYED INSERT DELAYED INTO table1 set field1= … INSERT DELAYED INTO，是客户端提交数据给MySQL，MySQL返回OK状态给客户端。而这是并不是已经将数据插入表，而是存储在内存里面等待排队。当mysql有空余时，再插入。另一个重要的好处是，来自许多客户端的插入被集中在一起，并被编写入一个块。这比执行许多独立的插入要快很多。坏处是，不能返回自动递增的ID，以及系统崩溃时，MySQL还没有来得及插入数据的话，这些数据将会丢失。 强制连接顺序 STRAIGHT_JOIN SELECT TABLE1.FIELD1, TABLE2.FIELD2 FROM TABLE1 STRAIGHT_JOIN TABLE2 WHERE … 由上面的SQL语句可知，通过STRAIGHT_JOIN强迫MySQL按TABLE1、TABLE2的顺序连接表。如果你认为按自己的顺序比MySQL推荐的顺序进行连接的效率高的话，就可以通过STRAIGHT_JOIN来确定连接顺序。 强制使用临时表 SQL_BUFFER_RESULT SELECT SQL_BUFFER_RESULT FROM TABLE1 WHERE … 当我们查询的结果集中的数据比较多时，可以通过SQL_BUFFER_RESULT.选项强制将结果集放到临时表中，这样就可以很快地释放MySQL的表锁（这样其它的SQL语句就可以对这些记录进行查询了），并且可以长时间地为客户端提供大记录集。 分组使用临时表 SQL_BIG_RESULT和SQL_SMALL_RESULT SELECT SQL_BUFFER_RESULT FIELD1, COUNT() FROM TABLE1 GROUP BY FIELD1; 一般用于分组或DISTINCT关键字，这个选项通知MySQL，如果有必要，就将查询结果放到临时表中，甚至在临时表中进行排序。SQL_SMALL_RESULT比起SQL_BIG_RESULT差不多，很少使用。 "},"olapyu-oltp-fen-xi.html":{"url":"olapyu-oltp-fen-xi.html","title":"OLAP与OLTP分析","keywords":"","body":" OLAP: 联机分析处理（OLAP）系统是数据仓库系统最主要的应用，专门设计用于支持复杂的分析操作，侧重对决策人员和高层管理人员的决策支持。 OLTP: 联机事务处理(OLTP，On-line Transaction Processing)应用，它所存储的数据被称为操作数据或者业务数据。 所以从定位上来讲，OLAP的定位是用来做数据分析(类BI),OLTP适合做一些事务的类的数据管理如查询如订单数据的产生。 举个通俗的例子，一个小规模的电商网站，会有下单的流程，那么这个下单流程产生的订单会是在OLTP数据库中，而如果电商的CEO想看本个月的运营情况，如果订单统计，理论上是应该在OLAP数据库（或者仓库）。 所以从本质上来讲，OLAP是读为主而OLTP以写为主。 然后，我们在来做一个基本的分析，就是常见的分析方式： Ad-hoc query：即席查询（Ad Hoc）是用户根据自己的需求，灵活的选择查询条件，系统能够根据用户的选择生成相应的统计报表。 固定字段分析：即用户的查询条件是固定的，我们可以按照定义好的字段进行报表提供，如周报、月报 关键字查询：如，用户的地址为 北京市朝阳区XXXXX，那么提供按照北京市XXX为关键字的检索查询 统计类查询：如生成一些箱图，热力图等 可以简单分析一下就是，在OLTP中，合理设计的情况下会存在1，3类查询，而在OLAP中会在1,2,3,4类查询。 接下来，我们分析一下传统技术的问题： 大家知道，不管在牛逼的系统，都逃不开硬件的限制，如磁盘IO、内存、CPU（往往也是大家忽略的）、网络IO。一般SATA硬盘的读写速度是在50~75M之间，普通网络均为千兆交换机，即100M传输速度。 那我们在来分析一下，数据库的特性：（本文章不讨论数据库的具体实现） 数据库能进行较快查询的原因是因为索引（及缓存）的存在，不同数据库的索引实现结构会稍微不太一样。索引也需要维护。 再结合我们之前讲到的分析，大家可以认为数据库在查询上的性能其实还是比较容易实现优化（结合数据库缓存），但是大家需要注意的是，如果查询的时候同时存在聚合（group by，sum，count），那么压力就会落在IO上，比如排序（因为单机内存有限，必须通过硬盘来实现排序） 这个时候压力就会落到IO上（请回顾上文提到的性能），所以当我们需要返回的数据条数越大（尤其分页），那么数据库就会变的非常非常的慢。 很多人会用数据库来进行数据清洗，也是因为IO的问题，导致变慢 大家不能忽略：当数据不大时，也会出现分析很慢的问题，是因为CPU计算能力有限的问题。 所以综合我的分析，大家可以得出几个结论： 数据库的问题在计算资源的有限 本身也没有支持关键字查询的方式（搜索引擎）。 主要是在查询+统计的场景下，数据库会有问题，其实本质来讲Ad-hoc query 如果没有统计的话，咱们通过分库+hash的方式是可以做到非常快的。 3.目前开源大数据方案，是否Ready？ 接下来，我通过工作中使用的一些技术给大家做一些分析，希望大家能对这个东西的解决方案有一些了解 我们在几个方面做比较，架构、效率、成熟度、学习难度等。 Hadoop+Hive+Tez： 架构 ： Hive 目前是Hadoop上的数据仓库，底层的技术为Tez(DAG MapReduce)，采用Yarn 作为资源管理平台，提供类SQL 接口，HQL,采用数据库作为元数据管理工具。 成熟度：Hive目前已经被非常多的人来使用，所以整体比较成熟。 效率：Hive目前结合Orcfile+压缩整体还是比较快的，但是也没有达到一些ad-hoc query要求的3秒内返回 学习难度 ：HQL，Hadoop 入门的难度都不高，所以学习曲线比较简单。 总结如下： Hive 目前这个软件适合做OLAP数据仓库类分析、数据清洗等对实时性要求不高的场景 Hive 不支持按照关键词查询，所以不能做搜索 Hive 索引比较弱，达不到数据库的性能。 Hive 不能满足3秒，5秒类似的快速返回的Ad-hoc query（即便将HDFS数据加入内存） 有Insert,update 等初级事务操作，所以可以认为未来可能可以做oltp。 Spark+Hadoop： 架构：Spark 技术中有一个比较好的技术就是-Spark SQL，这个技术可以实现使用SQL来操作Spark的RDD，当然Spark SQL最终也是要通过Spark的引擎，来使用所以最终会转换成Spark的MapReduce。 成熟度 ：目前仍是告诉发展。 效率： 整体比Hive率高，但是如果数据量非常大，没有特别好的效果。数据加入内存后查询（无统计）非常快。 学习曲线：需要学习Hadoop，Spark等，比较陡峭。 总结如下： 数据量不是特别大，完全装入内存，可以提供秒内的非统计类查询。不能完全装入内存的统计分析，结果与hive+tez的组合不会差太多，也不会领先特别多。 适合一定的Ad-hoc query场景与Olap 场景，不能做oltp 没有索引，无法做精确的查找，都是暴利扫描。 Impala+Hadoop： 架构：Impala技术目前性能不错，抛弃了MapReduce设计，结合HDFS缓存可以做更好的性能提高 成熟度：比较成熟 效率：配合Parquet ,性能与Hive+Tez接近，因为不需要启动在一定程度分析比Hive快 学习曲线：学习SQL与Impala 本身，所以难度一般。 总结： Impala性能不错，但是在大数据排序上，需要限制返回的行数，大表间Join也是个问题。 适合一定的Ad-hoc query场景与Olap 场景，不能做oltp 没有索引，无法做精确的查找，都是暴利扫描。 所以综合看，目前开源的大数据SQL方案，没有一个是完美的，都是或多或少的缺陷，我们需要由搜索引擎+nosql+redis等方案配合，来完成很多的场景。 我们需要对性能有一个结论：要求5秒内的，基本不适合用这种大数据SQL方案手段来做。需要借助更昂贵的数据库，或者等待开源技术成熟。 "},"a.html":{"url":"a.html","title":"A","keywords":"","body":""},"exists.html":{"url":"exists.html","title":"EXISTS和NOT EXISTS","keywords":"","body":" EXISTS 用于检查子查询是否至少会返回一行数据，该子查询实际上并不返回任何数据，而是返回值True或False，也就是EXISTS使用一个子查询来检测行的存在。 NOT EXISTS 的作用与 EXISTS 正好相反。如果子查询没有返回行，则满足了 NOT EXISTS 中的 WHERE 子句。 语法： EXISTS subquery 参数： subquery 是一个受限的 SELECT 语句 (不允许有 COMPUTE 子句和 INTO 关键字)。 结果类型： Boolean 如果子查询包含行，则返回 TRUE ，否则返回 FLASE 。 将外查询表的每一行代入内查询作为检验，如果内查询返回的结果取非空值，则EXISTS子句返回TRUE，这一行行可作为外查询的结果行，否则不能作为结果 过滤掉子查询中中包含NULL值，null代表什么都不是所以比较时候会排出掉 SELECT * FROM db0.test a '1','1','1' '2','3','2' '3','5','3' '5','6','4' NULL,'null','5' NULL,'123','6' SELECT * FROM db0.test a where exists (SELECT * FROM db0.test b where a.col1=b.col1) '1','1','1' '2','3','2' '3','5','3' '5','6','4' "},"null.html":{"url":"null.html","title":"NUll与not NULL","keywords":"","body":"空值” 和 “NULL” 的概念 空值是不占用空间的 mysql中的NULL其实是占用空间的,它是一个占空间什么都不是值 CREATE TABLE `test` ( `col1` varchar(10) DEFAULT NULL, `col2` varchar(10) CHARACTER SET utf8 DEFAULT NULL, `id` int(11) NOT NULL AUTO_INCREMENT, PRIMARY KEY (`id`), UNIQUE KEY `col1_UNIQUE` (`col1`), KEY `index1` (`col2`) ) ENGINE=InnoDB AUTO_INCREMENT=6 DEFAULT CHARSET=latin1 --不容许插入NUll INSERT INTO `test` VALUES (null,1); #1048 Column 'col1' cannot be null --可以插入空值 INSERT INTO `test` VALUES ('',1); NULL影响 要尽可能地把字段定义为 NOT NULL，即使应用程序无须保存 NULL（没有值），也有许多表包含了可空列（Nullable Column） 这仅仅是因为它为默认选项。除非真的要保存 NULL，否则就把列定义为 NOT NULL，把 NULL 列改为 NOT NULL 带来的性能提升很小，所以除非确定它引入了问题，否则就不要把它当作优先的优化措施。 然后，如果计划对列进行索引，就要尽量避免把它设置为可空,虽然在mysql里 Null值的列也是走索引的 如--col1中存在两条null行，走索引 explain SELECT * FROM db0.test a where a.col1 is null ; '1','SIMPLE','a',NULL,'ref','col1_UNIQUE','col1_UNIQUE','13','const','2','100.00','Using index condition' count 不统计null,也就是聚集类函数不计算 SELECT count(id) FROM db0.test; -- 6 SELECT count(col1) FROM db0.test; -- 4 in,exists在比较时候会排出掉null值的列，也就是不包含在返回结果中 "},"explain.html":{"url":"explain.html","title":"执行计划(EXPLAIN)","keywords":"","body":"EXPLAIN命令是查看查询优化器如何决定执行查询的主要方法,该命令会在查询上设置一个标记,当执行查询时,这个标记会使其返回关于在执行计划中每一步的信息,而不是执行它,它会返回一行或多行信息,显示出执行计划中的每一部分和执行的次序,从而可以从分析结果中找到查询语句或是表结构的性能瓶颈 如下： 分析出表的读取顺序 数据读取操作的操作类型 哪些索引可以使用 哪些索引被实际使用 表之间的引用 每张表有多少行被优化器查询 EXPLAIN SELECT …… --将执行计划\"反编译\"成SELECT语句，运行SHOW WARNINGS EXPLAIN EXTENDED SELECT …… -- SHOW WARNINGS --用于分区表的EXPLAIN生成QEP的信息执行计划包含的信息 EXPLAIN PARTITIONS SELECT …… id 包含一组数字，表示查询中执行select子句或操作表的顺序（id相同，执行顺序由上至下）子查询，id的序号会递增，id值越大优先级越高，越先被执行 explain select t2.* from (select t3.id from t3 where t3.name='')s1, t2 where s1.id=t2.id; +----+-------------+------------+--------+---------------+---------+---------+-------+------+--------------------------+ | id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra | +----+-------------+------------+--------+---------------+---------+---------+-------+------+--------------------------+ | 1 | PRIMARY | | system | NULL | NULL | NULL | NULL | 1 | | | 1 | PRIMARY | t2 | const | PRIMARY | PRIMARY | 4 | const | 1 | | | 2 | DERIVED | t3 | ref | name | name | 63 | | 1 | Using where; Using index | +----+-------------+------------+--------+---------------+---------+---------+-------+------+--------------------------+ 3 rows in set (0.00 sec) id如果相同，可以认为是一组，从上往下顺序执行；在所有组中，id值越大，优先级越高，越先执行 select_type SIMPLE：查询中不包含子查询或者UNION PRIMARY 查询中若包含任何复杂的子部分，最外层查询则被标记为：PRIMARY SELECT或WHERE列表中包含了子查询，该子查询被标记为：SUBQUERY 在FROM列表中包含的子查询被标记为：DERIVED（衍生）用来表示包含在from子句中的子查询的select，mysql会递归执行并将结果放到一个临时表中。服务器内部称为\"派生表\"，因为该临时表是从子查询中派生出来的 若第二个SELECT出现在UNION之后，则被标记为UNION；若UNION包含在FROM子句的子查询中，外层SELECT将被标记为：DERIVED 从UNION表获取结果的SELECT被标记为：UNION RESULT SUBQUERY和UNION还可以被标记为DEPENDENT和UNCACHEABLE。DEPENDENT意味着select依赖于外层查询中发现的数据。 UNCACHEABLE意味着select中的某些 特性阻止结果被缓存于一个item_cache中。 explain select d1.name, ( select id from t3) d2 from (select id,name from t1 where name='')d1 union (select name,id from t2); +----+--------------+------------+--------+---------------+------+---------+------+------+--------------------------+ | id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra | +----+--------------+------------+--------+---------------+------+---------+------+------+--------------------------+ | 1 | PRIMARY | | system | NULL | NULL | NULL | NULL | 0 | const row not found | | 3 | DERIVED | t1 | ref | name | name | 63 | | 1 | Using where; Using index | | 2 | SUBQUERY | t3 | index | NULL | age | 5 | NULL | 6 | Using index | | 4 | UNION | t2 | index | NULL | name | 63 | NULL | 4 | Using index | | NULL | UNION RESULT | | ALL | NULL | NULL | NULL | NULL | NULL | | +----+--------------+------------+--------+---------------+------+---------+------+------+--------------------------+ 5 rows in set (0.00 sec) id列为1，表示第一个select，select_type列的primary表 示该查询为外层查询，table列被标记为，表示查询结果来自一个衍生表，其中3代表该查询衍生自第三个select查询，即id为3的select。 id为3，表示该查询的执行次序为2（ 4 => 3），是整个查询中第三个select的一部分。因查询包含在from中，所以为derived。 select列表中的子查询，select_type为subquery，为整个查询中的第二个select。 select_type为union，说明第四个select是union里的第二个select，最先执行。 代表从union的临时表中读取行的阶段，table列的表示用第一个和第四个select的结果进行union操作。 type 表示MySQL在表中找到所需行的方式，又称“访问类型”，常见类型如下: ALL, index, range, ref, eq_ref, const, system, NULL从左到右，性能从最差到最好 possible_keys 指出MySQL能使用哪个索引在表中找到记录，查询涉及到的字段上若存在索引，则该索引将被列出，但不一定被查询使用,key才是实际使用 key 显示MySQL在查询中实际使用的索引，若没有使用索引，显示为NULL explain select id,age from t1; +----+-------------+-------+-------+---------------+------+---------+------+------+-------------+ | id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra | +----+-------------+-------+-------+---------------+------+---------+------+------+-------------+ | 1 | SIMPLE | t1 | index | NULL | age | 5 | NULL | 4 | Using index | +----+-------------+-------+-------+---------------+------+---------+------+------+-------------+ 1 row in set (0.00 sec) key_len 表示索引中使用的字节数，可通过该列计算查询中使用的索引的长度（key_len显示的值为索引字段的最大可能长度，并非实际使用长度，即key_len是根据表定义计算而得，不是通过表内检索出的） ref 表示上述表的连接匹配条件，即哪些列或常量被用于查找索引列上的值 rows 表示MySQL根据表统计信息及索引选用情况，估算的找到所需的记录所需要读取的行数 -- ref t1.id=t2.id 用于查找索引列上的值 explain select * from t1 , t2 where t1.id=t2.id and t2.name='atlas'; +----+-------------+-------+--------+---------------+---------+---------+------------+------+-------------+ | id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra | +----+-------------+-------+--------+---------------+---------+---------+------------+------+-------------+ | 1 | SIMPLE | t2 | ref | PRIMARY,name | name | 63 | const | 1 | Using where | | 1 | SIMPLE | t1 | eq_ref | PRIMARY | PRIMARY | 4 | test.t2.id | 1 | | +----+-------------+-------+--------+---------------+---------+---------+------------+------+-------------+ 2 rows in set (0.00 sec) Extra extra列中出现的信息一般不是太重要，但是还是有很多信息我们可以从这里面获取到： using index：出现这个说明mysql使用了覆盖索引，避免访问了表的数据行，效率不错！ using where： 这意味着MySQL服务器将在存储引擎检索行后再进行过滤。许多WHERE条件里涉及索引中的列，当（并且如果）它读取索引时，就能被存储引擎检验，因此不是所有带WHERE子句的查询都会显示“Using where”。有时“Usingwhere”的出现就是一个暗示：查询可受益于不同的索引。 using temporary：这意味着mysql对查询结果进行排序的时候使用了一张临时表。 using filesort：这个说明mysql会对数据使用一个外部的索引排序，而不是按照表内的索引顺序进行读取。 • EXPLAIN不会告诉你关于触发器、存储过程的信息或用户自定义函数对查询的影响情况 • EXPLAIN不考虑各种Cache • EXPLAIN不能显示MySQL在执行查询时所作的优化工作 • 部分统计信息是估算的，并非精确值 • EXPALIN只能解释SELECT操作，其他操作要重写为SELECT后查看执行计划。 "},"explain/type.html":{"url":"explain/type.html","title":"type","keywords":"","body":"如果查询没有办法找到合适的访问类型， 那么解决的最 办法通常就是增加一个合适的索引 官方的说法，说这列表示的是“访问类型”，更通俗一点就是:mysql找到需要的数据行的方式。一下就是从效率最差到最好顺序分别介绍下： ALL：Full Table Scan， MySQL将遍历全表以找到匹配的行 index：Full Index Scan，index与ALL区别为index类型只遍历索引树 explain select id from t1; +----+-------------+-------+-------+---------------+------+---------+------+------+-------------+ | id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra | +----+-------------+-------+-------+---------------+------+---------+------+------+-------------+ | 1 | SIMPLE | t1 | index | NULL | age | 5 | NULL | 4 | Using index | +----+-------------+-------+-------+---------------+------+---------+------+------+-------------+ 1 row in set (0.00 sec) range:索引范围扫描，对索引的扫描开始于某一点，返回匹配值域的行。显而易见的索引范围扫描是带有between或者where子句里带有查询。当mysql使用索引去查找一系列值时，例如IN()和OR列表，也会显示range（范围扫描）,当然性能上面是有差异的。 explain select * from t1 where id in (1,4); --或者 explain select * from t1 where id between 1 and 4; +----+-------------+-------+-------+---------------+---------+---------+------+------+-------------+ | id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra | +----+-------------+-------+-------+---------------+---------+---------+------+------+-------------+ | 1 | SIMPLE | t1 | range | PRIMARY | PRIMARY | 4 | NULL | 3 | Using where | +----+-------------+-------+-------+---------------+---------+---------+------+------+-------------+ 1 row in set (0.00 sec) explain select * from t1 where id=1 or id=4; +----+-------------+-------+-------+---------------+---------+---------+------+------+-------------+ | id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra | +----+-------------+-------+-------+---------------+---------+---------+------+------+-------------+ | 1 | SIMPLE | t1 | range | PRIMARY | PRIMARY | 4 | NULL | 2 | Using where | +----+-------------+-------+-------+---------------+---------+---------+------+------+-------------+ 1 row in set (0.01 sec) explain select * from t1 where id > 1; +----+-------------+-------+-------+---------------+---------+---------+------+------+-------------+ | id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra | +----+-------------+-------+-------+---------------+---------+---------+------+------+-------------+ | 1 | SIMPLE | t1 | range | PRIMARY | PRIMARY | 4 | NULL | 3 | Using where | +----+-------------+-------+-------+---------------+---------+---------+------+------+-------------+ 1 row in set (0.00 sec) ref：使用非唯一索引扫描或者唯一索引的前缀扫描，返回匹配某个单独值的记录行 explain select * from t1 where name='yayun'; +----+-------------+-------+------+---------------+------+---------+-------+------+-------------+ | id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra | +----+-------------+-------+------+---------------+------+---------+-------+------+-------------+ | 1 | SIMPLE | t1 | ref | name | name | 63 | const | 1 | Using where | +----+-------------+-------+------+---------------+------+---------+-------+------+-------------+ 1 row in set (0.00 sec) eq_ref：类似ref，区别就在使用的索引是唯一索引，对于每个索引键值，表中只有一条记录匹配，简单来说，就是多表连接中使用primary key或者 unique key作为关联条件 explain select t1.name from t1, t2 where t1.id=t2.id; +----+-------------+-------+--------+---------------+---------+---------+------------+------+-------------+ | id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra | +----+-------------+-------+--------+---------------+---------+---------+------------+------+-------------+ | 1 | SIMPLE | t1 | index | PRIMARY | name | 63 | NULL | 4 | Using index | | 1 | SIMPLE | t2 | eq_ref | PRIMARY | PRIMARY | 4 | test.t1.id | 1 | Using index | +----+-------------+-------+--------+---------------+---------+---------+------------+------+-------------+ 2 rows in set (0.00 sec) const、system：当MySQL对查询某部分进行优化，并转换为一个常量时，使用这些类型访问。 如将主键置于where列表中，MySQL就能将该查询转换为一个常量-- b1 就是一个常量 explain select * from ( select * from t1 where id=1)b1; +----+-------------+------------+--------+---------------+---------+---------+------+------+-------+ | id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra | +----+-------------+------------+--------+---------------+---------+---------+------+------+-------+ | 1 | PRIMARY | | system | NULL | NULL | NULL | NULL | 1 | | | 2 | DERIVED | t1 | const | PRIMARY | PRIMARY | 4 | | 1 | | +----+-------------+------------+--------+---------------+---------+---------+------+------+-------+ 2 rows in set (0.00 sec) system是const类型的特例，当查询的表只有一行的情况下，使用system NULL：MySQL在优化过程中分解语句，执行时甚至不用访问表或索引， 例如从一个索引列里选取最小值可以通过单独索引查找完成。 --min(id) explain select * from t1 where id = (select min(id) from t2); +----+-------------+-------+-------+---------------+---------+---------+-------+------+------------------------------+ | id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra | +----+-------------+-------+-------+---------------+---------+---------+-------+------+------------------------------+ | 1 | PRIMARY | t1 | const | PRIMARY | PRIMARY | 4 | const | 1 | | | 2 | SUBQUERY | NULL | NULL | NULL | NULL | NULL | NULL | NULL | Select tables optimized away | +----+-------------+-------+-------+---------------+---------+---------+-------+------+------------------------------+ 2 rows in set (0.00 sec) "},"explain/extra.html":{"url":"explain/extra.html","title":"extra","keywords":"","body":" Using index 该值表示相应的select操作中使用了覆盖索引 explain select id from t1; +----+-------------+-------+-------+---------------+------+---------+------+------+-------------+ | id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra | +----+-------------+-------+-------+---------------+------+---------+------+------+-------------+ | 1 | SIMPLE | t1 | index | NULL | age | 5 | NULL | 4 | Using index | +----+-------------+-------+-------+---------------+------+---------+------+------+-------------+ 1 row in set (0.00 sec) Using where 表示mysql服务器将在存储引擎检索行后再进行过滤。许多where条件里涉及索引中的列，当（并且如果）它读取索引时，就能被存储引擎检验，因此不是所有带where字句的查询都会显示\"Using where\"。有时\"Using where\"的出现就是一个暗示：查询可受益与不同的索引。 explain select id,name from t1 where id Using temporary 表示MySQL需要使用临时表来存储结果集，常见于排序和分组查询 这个值表示使用了内部临时(基于内存的)表。一个查询可能用到多个临时表。有很多原因都会导致MySQL在执行查询期间创建临时表。两个常见的原因是在来自不同表的上使用了DISTINCT,或者使用了不同的ORDER BY和GROUP BY列。可以强制指定一个临时表使用基于磁盘的MyISAM存储引擎。这样做的原因主要有两个： 1)内部临时表占用的空间超过min(tmp_table_size，max_heap_table_size)系统变量的限制 2)使用了TEXT/BLOB 列 explain select id from t1 where id in (1,2) group by age,name; +----+-------------+-------+-------+---------------+---------+---------+------+------+----------------------------------------------+ | id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra | +----+-------------+-------+-------+---------------+---------+---------+------+------+----------------------------------------------+ | 1 | SIMPLE | t1 | range | PRIMARY | PRIMARY | 4 | NULL | 2 | Using where; Using temporary; Using filesort | +----+-------------+-------+-------+---------------+---------+---------+------+------+----------------------------------------------+ 1 row in set (0.00 sec) Using filesort MySQL中无法利用索引完成的排序操作称为“文件排序” explain select id,age from t1 order by name; +----+-------------+-------+------+---------------+------+---------+------+------+----------------+ | id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra | +----+-------------+-------+------+---------------+------+---------+------+------+----------------+ | 1 | SIMPLE | t1 | ALL | NULL | NULL | NULL | NULL | 4 | Using filesort | +----+-------------+-------+------+---------------+------+---------+------+------+----------------+ 1 row in set (0.00 sec) explain select id,age from t1 order by age; +----+-------------+-------+-------+---------------+------+---------+------+------+-------------+ | id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra | +----+-------------+-------+-------+---------------+------+---------+------+------+-------------+ | 1 | SIMPLE | t1 | index | NULL | age | 5 | NULL | 4 | Using index | +----+-------------+-------+-------+---------------+------+---------+------+------+-------------+ 1 row in set (0.00 sec) Using join buffer 改值强调了在获取连接条件时没有使用索引，并且需要连接缓冲区来存储中间结果。如果出现了这个值，那应该注意，根据查询的具体情况可能需要添加索引来改进能。 explain select t1.name from t1 inner join t2 on t1.name=t2.name; +----+-------------+-------+-------+---------------+------+---------+--------------+------+--------------------------+ | id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra | +----+-------------+-------+-------+---------------+------+---------+--------------+------+--------------------------+ | 1 | SIMPLE | t1 | index | name | name | 63 | NULL | 4 | Using index | | 1 | SIMPLE | t2 | ref | name | name | 63 | test.t1.name | 2 | Using where; Using index | +----+-------------+-------+-------+---------------+------+---------+--------------+------+--------------------------+ 2 rows in set (0.00 sec) alter table t1 drop key name; Query OK, 0 rows affected (0.02 sec) Records: 0 Duplicates: 0 Warnings: 0 alter table t2 drop key name; Query OK, 0 rows affected (0.02 sec) Records: 0 Duplicates: 0 Warnings: 0 explain select t1.name from t1 inner join t2 on t1.name=t2.name; +----+-------------+-------+------+---------------+------+---------+------+------+--------------------------------+ | id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra | +----+-------------+-------+------+---------------+------+---------+------+------+--------------------------------+ | 1 | SIMPLE | t1 | ALL | NULL | NULL | NULL | NULL | 4 | | | 1 | SIMPLE | t2 | ALL | NULL | NULL | NULL | NULL | 4 | Using where; Using join buffer | +----+-------------+-------+------+---------------+------+---------+------+------+--------------------------------+ 2 rows in set (0.00 sec) Impossible where 这个值强调了where语句会导致没有符合条件的行。 EXPLAIN SELECT * FROM t1 WHERE 1=2; +----+-------------+-------+------+---------------+------+---------+------+------+------------------+ | id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra | +----+-------------+-------+------+---------------+------+---------+------+------+------------------+ | 1 | SIMPLE | NULL | NULL | NULL | NULL | NULL | NULL | NULL | Impossible WHERE | +----+-------------+-------+------+---------------+------+---------+------+------+------------------+ 1 row in set (0.00 sec) Select tables optimized away 这个值意味着仅通过使用索引，优化器可能仅从聚合函数结果中返回一行. explain select max(id) from t1; +----+-------------+-------+------+---------------+------+---------+------+------+------------------------------+ | id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra | +----+-------------+-------+------+---------------+------+---------+------+------+------------------------------+ | 1 | SIMPLE | NULL | NULL | NULL | NULL | NULL | NULL | NULL | Select tables optimized away | +----+-------------+-------+------+---------------+------+---------+------+------+------------------------------+ 1 row in set (0.00 sec) Index merges 当MySQL 决定要在一个给定的表上使用超过一个索引的时候，就会出现以下格式中的一个，详细说明使用的索引以及合并的类型。 Using sort_union(...) Using union(...) Using intersect(...) "},"join.html":{"url":"join.html","title":"JOIN与子查询","keywords":"","body":"子查询和JOIN 复杂的需求场景导致某些子查询场景不可避免。关于子查询，存在不少性能陷阱和认识误区值得关注。 1）MySQL子查询性能差的主要原因是子查询产生临时表吗？不完全正确，临时表并不可怕，一个完整的SQL语句，FROM/JOIN/GROUP/WHERE/ORDER等操作，不考虑索引优化的情况下，都有可能产生临时表。所以更严格的表述是在子查询产生的临时表上查询无法利用索引导致性能低下。 2）IN子查询往往性能不佳的真实原因是什么？是IN查询的临时表数据量太大，MySQL太弱，只能支持极少数量的IN子查询吗？不一定，显示列表IN（a,b,c）查询的性能并不算差，IN子查询真正的性能陷阱在于Mysql优化器往往将IN独立子查询优化成EXISTS相关子查询！所以当观察SELECT * FROM table1 WHERE table1.id IN(SELECT id FROM table2)的查询计划，会发现table2的查询为DEPEDENTSUBQUERY，原因其实是MySQL优化策略+历史原因。 3）子查询的性能一定弱于JOIN吗？未必，由于Mysql不支持Semi Join（注），所以在某些需要场景下，使用子查询性能优于JOIN。比如A表和B表一对多关系，如果仅仅想查询在B表中存在对应记录的A表记录，如果使用JOIN，需要用DISTINCT或者GROUP操作进行去重操作。使用关联子查询可以避免这部分开销。 SELECT id FROM table1 WHERE EXISTS(SELECT table2.id FROM table2 WHERE table2.id=table1.id) 关于Join，Mysql使用Nested Loop算法（注）。在典型的星型维度模型中，维度表数据量远小于事实表，JOIN操作往往是大小表连接，性能问题不大，这方面不多讲。结合前面提到的Covering Index， 介绍一个利用JOIN提高分页效率的歪招： 分页往往需要用到LIMIT OFFSET,在偏移量很大的时候，比如LIMIT 100000,50，MySQL需要检索100050数据，性能严重下降。常见的处理方式是 a）增加排序辅助列，将LIMIT转化为在辅助列上范围查找操作 b）应用层缓存机制 c）需求折中，没有人会翻到100000页。以上皆不灵的时候，可以选择Covering Index+Join。 SELECT * FROM table1 INNER JOIN (SELECT id FROM table1 ORDER BY indexed_col limit 100000,50) AS a ON table1.id = a.id 这种方式效率较高，因为临时表a仅在索引上进行操作（Innodb索引叶子节点上存储了主键值），取得所需行id之后，再和完整的表进行Join获取其他所需列。 "},"join/biao-lian-jie-suan-fa.html":{"url":"join/biao-lian-jie-suan-fa.html","title":"表连接算法","keywords":"","body":"Nested Loop Join（NLJ）算法： NLJ（嵌套循环算法） 循环外层是驱动表，循坏内层是被驱动表。驱动表会驱动被驱动表进行连接操作。 首先驱动表找到第一条记录，然后从头扫描被驱动表，逐一查找与驱动表第一条记录匹配的记录然后连接起来形成结果表中的一条记。被驱动表查找完后， 再从驱动表中取出第二个记录，然后从头扫描被驱动表，逐一查找与驱动表第二条记录匹配的记录，连接起来形成结果表中的一条记录。 重复上述操作，直到驱动表的全部记录都处理完毕为止。这就是嵌套循环连接算法的基本思想。 --伪代码 foreach row1 from t1 foreach row2 from t2 if row2 match row1 //row2与row1匹配，满足连接条件 join row1 and row2 into result //连接row1和row2加入结果集 首先加载t1，然后从t1中取出第一条记录，之后加载t2表，与t2表中的记录逐个匹配，连接匹配的记录 Block Nested Loop Join(BNLJ)算法： 再介绍一种高级算法：BNLJ，块嵌套循环算法，可以看作对NLJ的优化。 大致思想就是建立一个缓存区，一次从驱动表中取多条记录，然后扫描被驱动表，被驱动表的每一条记录都尝试与缓冲区中的多条记录匹配，如果匹配则连接并加入结果集。缓冲区越大，驱动表一次取出的记录就越多。 这个算法的优化思路就是减少内循环的次数从而提高表连接效率。 影响性能的因素 1.内循环的次数： 现在考虑这么一个场景，当t1有100条记录，t2有10000条记录。那么，t1驱动t2与t2驱动t1，他们之间在效率上孰优孰劣？ 如果是单纯的分析指令执行次数，他们都是100*10000,但是考虑到加载表的次数呢。 首先分析t1驱动t2，t1表加载1次，t2表需要加载100次。然后分析t2驱动t1，t2表首先加载1次，但是t1表要加载10000次。所以，t1驱动t2的效率要优于t2驱动t1的效率。由此得出，小表驱动大表能够减少内循环的次数从而提高连接效率。 另外，如果使用Block Nested Loop Join算法的话，通过扩大一次缓存区的大小也能减小内循环的次数。由此又可得，设置合理的缓冲区大小能够提高连接效率 2.快速匹配：扫描被驱动表寻找合适的记录可以看做一个查询操作，如何提高查询的效率呢？建索引啊！由此还可得出，在被驱动表建立索引能够提高连接效率 3.排序：假设t1表驱动t2表进行连接操作，连接条件是t1.id=t2.id，而且要求查询结果对id排序。现在有两种选择，方式一[...ORDER BY t1.id]，方式二[...ORDER BY t2.id]。如果我们使用方式一的话，可以先对t1进行排序然后执行表连接算法，如果我们使用方式二的话，只能在执行表连接算法后，对结果集进行排序（Using temporary），效率自然低下。由此最后可得出，优先选择驱动表的属性进行排序能够提高连接效率。 "},"join/join.html":{"url":"join/join.html","title":"Join","keywords":"","body":"JOIN称为连接，连接的主要作用是根据两个或多个表中的列之间的关系，获取存在于不同表中的数据。连接分为如下： 笛卡尔积 设A,B为集合，用A中元素为第一元素，B中元素为第二元素构成有序对，所有这样的有序对组成的集合叫做A与B的笛卡尔积，记作AxB. 笛卡尔积的符号化为： A×B={(x,y)|x∈A∧y∈B} 例如，A={a,b}, B={0,1,2}，则 A×B={(a, 0), (a, 1), (a, 2), (b, 0), (b, 1), (b, 2)} B×A={(0, a), (0, b), (1, a), (1, b), (2, a), (2, b)} -- t1 与t2 进行笛卡尔积运算 t1 * t2,也就是在t1中包含每个列元素需要与t2中所有元素合成行 INSERT INTO t1(id, person) VALUES(1,'小明'), (2,'小红'), (3,'小强'); INSERT INTO t2(id, person) VALUES('A','小明'), ('B','小红'), ('C','小刚'); SELECT t1.id, t2.id FROM t1 CROSS JOIN t2; --笛卡尔积 '1','A','小明' '2','A','小明' '3','A','小明' '1','B','小红' '2','B','小红' '3','B','小红' '1','C','小刚' '2','C','小刚' '3','C','小刚' --或者 SELECT t1.id,t1.person, t2.id ,t2.person FROM t1 CROSS JOIN t2; '1','小明','A','小明' '2','小红','A','小明' '3','小强','A','小明' '1','小明','B','小红' '2','小红','B','小红' '3','小强','B','小红' '1','小明','C','小刚' '2','小红','C','小刚' '3','小强','C','小刚' -- t1 返回的每行与t2返回所有行进行组合 INNER JOIN （内连接） 就是求两个表的交集，从笛卡尔积的角度讲就是从笛卡尔积中选出满足某条件的记录 -- 在t1* t2 笛卡尔积中根据t1.person = t2.person（交集） 条件过滤出新的集 SELECT t1.id, t2.id FROM t1 INNER JOIN t2 ON t1.person = t2.person; --结果 '1','A' '2','B' t1.person = t2.person 就是两个表的交集 LEFT JOIN 和 RIGHT JOIN 是在INNER JOIN交集的基础上列出左或者右表中不交集的元素 SELECT t1.id, t2.id FROM t1 RIGHT JOIN t2 on t1.person = t2.person -- 在t1 与t2的交集中在列出右边表中不交集的元素 '1','A' '2','B' NULL,'C' 外连接 全连接 "},"in.html":{"url":"in.html","title":"in与not in","keywords":"","body":"in：包含 查询和所有女生年龄相同的男生 select * from stu where sex='男' and age in(select age from stu where sex='女') in()后面的子查询 是返回结果集的,换句话说执行次序和exists()不一样.子查询先产生结果集, 然后主查询再去结果集里去找符合要求的字段列表去.符合要求的输出,反之则不输出. "},"show.html":{"url":"show.html","title":"show","keywords":"","body":""},"show/variables.html":{"url":"show/variables.html","title":"variables","keywords":"","body":"variables like xxx 查看mysql运行时参数或者配置 查看MySQL服务器配置信息 show variables; 连接数 --最大连接数 show variables like 'max_connections'; +-----------------+-------+ | Variable_name | Value | +-----------------+-------+ | max_connections | 151 | --当前响应的连接数是 show global status like 'max_used_connections'; +----------------------+-------+ | Variable_name | Value | +----------------------+-------+ | Max_used_connections | 6 | +----------------------+-------+ --查询连接地址和时间 show full processlist ; thread_cache_size 线程缓存show variables like 'thread_cache_size'; +-------------------+-------+ | Variable_name | Value | +-------------------+-------+ | thread_cache_size | 8 | +-------------------+-------+ 如果我们在MySQL服务器配置文件中设置了thread_cache_size，当客户端断开之后，服务器处理此客户的线程将会缓存起来以响应下一个客户而不是销毁（前提是缓存数未达上限）。 threads_created表示创建过的线程数，如果发现Threads_created值过大的话，表明 MySQL服务器一直在创建线程，这也是比较耗资源，可以适当增加配置文件中thread_cache_size值 threads_created表示创建过的线程数 show global status like 'thread%'; 'Threads_cached', '8' 'Threads_connected', '2' 'Threads_created', '24' 'Threads_running', '1' 连接数理想数量 max_used_connections / max_connections * 100% = 99.6% （理想值 ≈ 85%） "},"show/status.html":{"url":"show/status.html","title":"status","keywords":"","body":"SHOW STATUS-查看MySQL服务器运行的各种状态值 SHOW STATUS命令会显示每个服务器变量的名字和值，状态变量是只读的。我们可以在MySQL客户端下运行SHOW STATUS或者在命令行运用mysqladmin extended-status来查看这些变量。如果使用SQL命令，可以使用LIKE或者WHERE来限制结果。LIKE可以对变量名做标准模式匹配。 SHOW STATUS中混杂了全局和会话变量，其中许多变量有双重域：既是全局变量，也是会话变量，有相同的名字。如果只需要看全局变量，需要改为SHOW GLOBAL STATUS查看。 --查看MySQL本次启动后的运行时间(单位：秒) show status like 'uptime'; --查看select语句的执行数 show [global] status like 'com_select'; --查看insert语句的执行数 show [global] status like 'com_insert'; --查看update语句的执行数 show [global] status like 'com_update'; --查看delete语句的执行数 show [global] status like 'com_delete'; --查看试图连接到MySQL(不管是否连接成功)的连接数 show status like 'connections'; --查看线程缓存内的线程的数量。 show status like 'threads_cached'; --查看当前打开的连接的数量。 show status like 'threads_connected'; --查看当前打开的连接的数量。 show status like 'threads_connected'; --查看创建用来处理连接的线程数。如果Threads_created较大，你可能要增加thread_cache_size值。 show status like 'threads_created'; --查看激活的(非睡眠状态)线程数。 show status like 'threads_running'; --查看立即获得的表的锁的次数。 show status like 'table_locks_immediate'; --查看不能立即获得的表的锁的次数。如果该值较高，并且有性能问题，你应首先优化查询，然后拆分表或使用复制。 show status like 'table_locks_waited'; --查看创建时间超过slow_launch_time秒的线程数。 show status like 'slow_launch_threads'; --查看查询时间超过long_query_time秒的查询的个数。 show status like 'slow_queries'; "},"deletehe-truncate-table.html":{"url":"deletehe-truncate-table.html","title":"delete和truncate table","keywords":"","body":"mysql中删除表记录delete from和truncate table delete from语句可以使用where对要删除的记录进行选择。delete语句更灵活。 truncate table将删除表中的所有记录。 delete和truncate table的最大区别是delete可以通过WHERE语句选择要删除的记录。但执行得速度不快。而且还可以返回被删除的记录数。而truncate table无法删除指定的记录，而且不能返回被删除的记录。但它执行得非常快。 "},"limit.html":{"url":"limit.html","title":"limit","keywords":"","body":""}}